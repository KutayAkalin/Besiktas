{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Advanced ML Group Project Final",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NQ43IddJolI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from sklearn.impute import SimpleImputer\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR4meK-ZJ-Ma",
        "colab_type": "text"
      },
      "source": [
        "# Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FEj4l2LJ47-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3223b464-4a1d-4cb8-855a-92fe85095a8f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUZXT1vxJ7aZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "34d0e094-50e2-439f-f215-d66959c49d7e"
      },
      "source": [
        "os.chdir(\"drive/My Drive/Colab Notebooks/\")\n",
        "data = pd.read_excel(\"AML_Group_Project_Dataset.xlsx\")\n",
        "data.head()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Participant_Stimuli</th>\n",
              "      <th>Hbo_Op1</th>\n",
              "      <th>Hbo_Op2</th>\n",
              "      <th>Hbo_Op3</th>\n",
              "      <th>Hbo_Op4</th>\n",
              "      <th>Hbo_Op5</th>\n",
              "      <th>Hbo_Op6</th>\n",
              "      <th>Hbo_Op7</th>\n",
              "      <th>Hbo_Op8</th>\n",
              "      <th>Hbo_Op9</th>\n",
              "      <th>Hbo_Op10</th>\n",
              "      <th>Hbo_Op11</th>\n",
              "      <th>Hbo_Op12</th>\n",
              "      <th>Hbo_Op13</th>\n",
              "      <th>Hbo_Op14</th>\n",
              "      <th>Hbo_Op15</th>\n",
              "      <th>Hbo_Op16</th>\n",
              "      <th>Hbr_Op1</th>\n",
              "      <th>Hbr_Op2</th>\n",
              "      <th>Hbr_Op3</th>\n",
              "      <th>Hbr_Op4</th>\n",
              "      <th>Hbr_Op5</th>\n",
              "      <th>Hbr_Op6</th>\n",
              "      <th>Hbr_Op7</th>\n",
              "      <th>Hbr_Op8</th>\n",
              "      <th>Hbr_Op9</th>\n",
              "      <th>Hbr_Op10</th>\n",
              "      <th>Hbr_Op11</th>\n",
              "      <th>Hbr_Op12</th>\n",
              "      <th>Hbr_Op13</th>\n",
              "      <th>Hbr_Op14</th>\n",
              "      <th>Hbr_Op15</th>\n",
              "      <th>Hbr_Op16</th>\n",
              "      <th>Hbt_Op1</th>\n",
              "      <th>Hbt_Op2</th>\n",
              "      <th>Hbt_Op3</th>\n",
              "      <th>Hbt_Op4</th>\n",
              "      <th>Hbt_Op5</th>\n",
              "      <th>Hbt_Op6</th>\n",
              "      <th>Hbt_Op7</th>\n",
              "      <th>Hbt_Op8</th>\n",
              "      <th>Hbt_Op9</th>\n",
              "      <th>Hbt_Op10</th>\n",
              "      <th>Hbt_Op11</th>\n",
              "      <th>Hbt_Op12</th>\n",
              "      <th>Hbt_Op13</th>\n",
              "      <th>Hbt_Op14</th>\n",
              "      <th>Hbt_Op15</th>\n",
              "      <th>Hbt_Op16</th>\n",
              "      <th>Oxy_Op1</th>\n",
              "      <th>Oxy_Op2</th>\n",
              "      <th>Oxy_Op3</th>\n",
              "      <th>Oxy_Op4</th>\n",
              "      <th>Oxy_Op5</th>\n",
              "      <th>Oxy_Op6</th>\n",
              "      <th>Oxy_Op7</th>\n",
              "      <th>Oxy_Op8</th>\n",
              "      <th>Oxy_Op9</th>\n",
              "      <th>Oxy_Op10</th>\n",
              "      <th>Oxy_Op11</th>\n",
              "      <th>Oxy_Op12</th>\n",
              "      <th>Oxy_Op13</th>\n",
              "      <th>Oxy_Op14</th>\n",
              "      <th>Oxy_Op15</th>\n",
              "      <th>Oxy_Op16</th>\n",
              "      <th>measurement</th>\n",
              "      <th>ResponseCode</th>\n",
              "      <th>TypeCode</th>\n",
              "      <th>AgeCode</th>\n",
              "      <th>SexCode</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2_1</td>\n",
              "      <td>0.024675</td>\n",
              "      <td>-0.015218</td>\n",
              "      <td>0.032464</td>\n",
              "      <td>0.058835</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.002910</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.045010</td>\n",
              "      <td>0.001022</td>\n",
              "      <td>0.052551</td>\n",
              "      <td>0.035209</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.052528</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.069685</td>\n",
              "      <td>-0.014196</td>\n",
              "      <td>0.085015</td>\n",
              "      <td>0.094044</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.055438</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.020335</td>\n",
              "      <td>-0.016239</td>\n",
              "      <td>-0.020087</td>\n",
              "      <td>0.023625</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.049618</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2_1</td>\n",
              "      <td>0.030511</td>\n",
              "      <td>-0.012349</td>\n",
              "      <td>0.039233</td>\n",
              "      <td>0.074926</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.003348</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.053890</td>\n",
              "      <td>0.000589</td>\n",
              "      <td>0.060653</td>\n",
              "      <td>0.040470</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.075030</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.084401</td>\n",
              "      <td>-0.011760</td>\n",
              "      <td>0.099886</td>\n",
              "      <td>0.115395</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.071682</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.023379</td>\n",
              "      <td>-0.012938</td>\n",
              "      <td>-0.021419</td>\n",
              "      <td>0.034456</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.078378</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2_1</td>\n",
              "      <td>0.039081</td>\n",
              "      <td>-0.004290</td>\n",
              "      <td>0.048589</td>\n",
              "      <td>0.094117</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.003982</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.058895</td>\n",
              "      <td>-0.002358</td>\n",
              "      <td>0.063509</td>\n",
              "      <td>0.041851</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.090856</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.097976</td>\n",
              "      <td>-0.006647</td>\n",
              "      <td>0.112098</td>\n",
              "      <td>0.135968</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.086874</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.019813</td>\n",
              "      <td>-0.001932</td>\n",
              "      <td>-0.014920</td>\n",
              "      <td>0.052266</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.094838</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2_1</td>\n",
              "      <td>0.050448</td>\n",
              "      <td>0.008479</td>\n",
              "      <td>0.060856</td>\n",
              "      <td>0.115959</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.003388</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.059176</td>\n",
              "      <td>-0.008715</td>\n",
              "      <td>0.060312</td>\n",
              "      <td>0.038532</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.095729</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.109625</td>\n",
              "      <td>-0.000236</td>\n",
              "      <td>0.121168</td>\n",
              "      <td>0.154490</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.099117</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.008728</td>\n",
              "      <td>0.017194</td>\n",
              "      <td>0.000544</td>\n",
              "      <td>0.077427</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.092341</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2_1</td>\n",
              "      <td>0.064245</td>\n",
              "      <td>0.024718</td>\n",
              "      <td>0.075855</td>\n",
              "      <td>0.139330</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.019458</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.054452</td>\n",
              "      <td>-0.018922</td>\n",
              "      <td>0.051060</td>\n",
              "      <td>0.030282</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.087221</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.118697</td>\n",
              "      <td>0.005796</td>\n",
              "      <td>0.126915</td>\n",
              "      <td>0.169612</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.106679</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.009794</td>\n",
              "      <td>0.043639</td>\n",
              "      <td>0.024794</td>\n",
              "      <td>0.109047</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.067763</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0 Participant_Stimuli   Hbo_Op1  ...  TypeCode  AgeCode  SexCode\n",
              "0           0                 2_1  0.024675  ...       1.0        1        1\n",
              "1           1                 2_1  0.030511  ...       1.0        1        1\n",
              "2           2                 2_1  0.039081  ...       1.0        1        1\n",
              "3           3                 2_1  0.050448  ...       1.0        1        1\n",
              "4           4                 2_1  0.064245  ...       1.0        1        1\n",
              "\n",
              "[5 rows x 71 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2-nak54KEVt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "01f2e57f-0a0f-4fd0-8252-695a70e15df8"
      },
      "source": [
        "print(data.shape)\n",
        "print(data.describe)\n",
        "print(data.info())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(81740, 71)\n",
            "<bound method NDFrame.describe of        Unnamed: 0 Participant_Stimuli   Hbo_Op1  ...  TypeCode  AgeCode  SexCode\n",
            "0               0                 2_1  0.024675  ...       1.0        1        1\n",
            "1               1                 2_1  0.030511  ...       1.0        1        1\n",
            "2               2                 2_1  0.039081  ...       1.0        1        1\n",
            "3               3                 2_1  0.050448  ...       1.0        1        1\n",
            "4               4                 2_1  0.064245  ...       1.0        1        1\n",
            "...           ...                 ...       ...  ...       ...      ...      ...\n",
            "81735       81735              172_30  0.047532  ...       2.0        1        0\n",
            "81736       81736              172_30  0.057649  ...       2.0        1        0\n",
            "81737       81737              172_30  0.066079  ...       2.0        1        0\n",
            "81738       81738              172_30  0.074707  ...       2.0        1        0\n",
            "81739       81739              172_30  0.085232  ...       2.0        1        0\n",
            "\n",
            "[81740 rows x 71 columns]>\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 81740 entries, 0 to 81739\n",
            "Data columns (total 71 columns):\n",
            " #   Column               Non-Null Count  Dtype  \n",
            "---  ------               --------------  -----  \n",
            " 0   Unnamed: 0           81740 non-null  int64  \n",
            " 1   Participant_Stimuli  81740 non-null  object \n",
            " 2   Hbo_Op1              63439 non-null  float64\n",
            " 3   Hbo_Op2              72898 non-null  float64\n",
            " 4   Hbo_Op3              76537 non-null  float64\n",
            " 5   Hbo_Op4              74916 non-null  float64\n",
            " 6   Hbo_Op5              68061 non-null  float64\n",
            " 7   Hbo_Op6              74610 non-null  float64\n",
            " 8   Hbo_Op7              75524 non-null  float64\n",
            " 9   Hbo_Op8              71325 non-null  float64\n",
            " 10  Hbo_Op9              74312 non-null  float64\n",
            " 11  Hbo_Op10             71016 non-null  float64\n",
            " 12  Hbo_Op11             71121 non-null  float64\n",
            " 13  Hbo_Op12             73384 non-null  float64\n",
            " 14  Hbo_Op13             68859 non-null  float64\n",
            " 15  Hbo_Op14             73427 non-null  float64\n",
            " 16  Hbo_Op15             55709 non-null  float64\n",
            " 17  Hbo_Op16             69956 non-null  float64\n",
            " 18  Hbr_Op1              63439 non-null  float64\n",
            " 19  Hbr_Op2              72898 non-null  float64\n",
            " 20  Hbr_Op3              76537 non-null  float64\n",
            " 21  Hbr_Op4              74916 non-null  float64\n",
            " 22  Hbr_Op5              68061 non-null  float64\n",
            " 23  Hbr_Op6              74610 non-null  float64\n",
            " 24  Hbr_Op7              75524 non-null  float64\n",
            " 25  Hbr_Op8              71325 non-null  float64\n",
            " 26  Hbr_Op9              74312 non-null  float64\n",
            " 27  Hbr_Op10             71016 non-null  float64\n",
            " 28  Hbr_Op11             71121 non-null  float64\n",
            " 29  Hbr_Op12             73384 non-null  float64\n",
            " 30  Hbr_Op13             68859 non-null  float64\n",
            " 31  Hbr_Op14             73427 non-null  float64\n",
            " 32  Hbr_Op15             55709 non-null  float64\n",
            " 33  Hbr_Op16             69956 non-null  float64\n",
            " 34  Hbt_Op1              63439 non-null  float64\n",
            " 35  Hbt_Op2              72898 non-null  float64\n",
            " 36  Hbt_Op3              76537 non-null  float64\n",
            " 37  Hbt_Op4              74916 non-null  float64\n",
            " 38  Hbt_Op5              68061 non-null  float64\n",
            " 39  Hbt_Op6              74610 non-null  float64\n",
            " 40  Hbt_Op7              75524 non-null  float64\n",
            " 41  Hbt_Op8              71325 non-null  float64\n",
            " 42  Hbt_Op9              74312 non-null  float64\n",
            " 43  Hbt_Op10             71016 non-null  float64\n",
            " 44  Hbt_Op11             71121 non-null  float64\n",
            " 45  Hbt_Op12             73384 non-null  float64\n",
            " 46  Hbt_Op13             68859 non-null  float64\n",
            " 47  Hbt_Op14             73427 non-null  float64\n",
            " 48  Hbt_Op15             55709 non-null  float64\n",
            " 49  Hbt_Op16             69956 non-null  float64\n",
            " 50  Oxy_Op1              63439 non-null  float64\n",
            " 51  Oxy_Op2              72898 non-null  float64\n",
            " 52  Oxy_Op3              76537 non-null  float64\n",
            " 53  Oxy_Op4              74916 non-null  float64\n",
            " 54  Oxy_Op5              68061 non-null  float64\n",
            " 55  Oxy_Op6              74610 non-null  float64\n",
            " 56  Oxy_Op7              75524 non-null  float64\n",
            " 57  Oxy_Op8              71325 non-null  float64\n",
            " 58  Oxy_Op9              74312 non-null  float64\n",
            " 59  Oxy_Op10             71016 non-null  float64\n",
            " 60  Oxy_Op11             71121 non-null  float64\n",
            " 61  Oxy_Op12             73384 non-null  float64\n",
            " 62  Oxy_Op13             68859 non-null  float64\n",
            " 63  Oxy_Op14             73427 non-null  float64\n",
            " 64  Oxy_Op15             55709 non-null  float64\n",
            " 65  Oxy_Op16             69956 non-null  float64\n",
            " 66  measurement          81740 non-null  int64  \n",
            " 67  ResponseCode         77676 non-null  float64\n",
            " 68  TypeCode             81256 non-null  float64\n",
            " 69  AgeCode              81740 non-null  int64  \n",
            " 70  SexCode              81740 non-null  int64  \n",
            "dtypes: float64(66), int64(4), object(1)\n",
            "memory usage: 44.3+ MB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnOCnc_fKGhm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "62bb513e-e7f0-463b-e89e-96d63b402253"
      },
      "source": [
        "pd.DataFrame(data.isna().sum())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Participant_Stimuli</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Hbo_Op1</th>\n",
              "      <td>18301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Hbo_Op2</th>\n",
              "      <td>8842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Hbo_Op3</th>\n",
              "      <td>5203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>measurement</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ResponseCode</th>\n",
              "      <td>4064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TypeCode</th>\n",
              "      <td>484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AgeCode</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SexCode</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>71 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                         0\n",
              "Unnamed: 0               0\n",
              "Participant_Stimuli      0\n",
              "Hbo_Op1              18301\n",
              "Hbo_Op2               8842\n",
              "Hbo_Op3               5203\n",
              "...                    ...\n",
              "measurement              0\n",
              "ResponseCode          4064\n",
              "TypeCode               484\n",
              "AgeCode                  0\n",
              "SexCode                  0\n",
              "\n",
              "[71 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8l6weRbKH06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = \"{:.3f}\".format\n",
        "from google.colab import widgets\n",
        "\n",
        "from IPython.core.display import display, HTML\n",
        "import base64\n",
        "!pip install facets-overview==1.0.0\n",
        "from facets_overview.feature_statistics_generator import FeatureStatisticsGenerator\n",
        "\n",
        "fsg = FeatureStatisticsGenerator()\n",
        "dataframes = [\n",
        "    {'table': data, 'name': 'Data'}]\n",
        "censusProto = fsg.ProtoFromDataFrames(dataframes)\n",
        "protostr = base64.b64encode(censusProto.SerializeToString()).decode(\"utf-8\")\n",
        "\n",
        "\n",
        "HTML_TEMPLATE = \"\"\"<script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"></script>\n",
        "        <link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html\">\n",
        "        <facets-overview id=\"elem\"></facets-overview>\n",
        "        <script>\n",
        "          document.querySelector(\"#elem\").protoInput = \"{protostr}\";\n",
        "        </script>\"\"\"\n",
        "html = HTML_TEMPLATE.format(protostr=protostr)\n",
        "display(HTML(html))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBvjNXP7KKqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.rcParams['figure.figsize'] = (20.0, 10.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVZDUhW7KL4g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(1,33): \n",
        "    plt.subplot(8, 4, i)\n",
        "    plt.tight_layout()\n",
        "    plt.hist(data.iloc[:,i],30)\n",
        "    plt.title(data.columns[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSDSe56IKNte",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(33,65): \n",
        "    plt.subplot(8, 4, i-32)\n",
        "    plt.tight_layout()\n",
        "    plt.hist(data.iloc[:,i],30)\n",
        "    plt.title(data.columns[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARENuWfPKQeO",
        "colab_type": "text"
      },
      "source": [
        "## Data Imputation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wah6DdGFgCsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#deleting response nan\n",
        "data = data.dropna(,axis=0).reset_index()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXrf_vlweHoZ",
        "colab_type": "text"
      },
      "source": [
        "### Deneme 1 (z-score)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JELaZzBOg7F5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "1fc0cf34-c78a-4f33-ab20-9ce27a640684"
      },
      "source": [
        "pd.DataFrame(data.isna().sum()).iloc[50:,0]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Oxy_Op1            0\n",
              "Oxy_Op2            0\n",
              "Oxy_Op3            0\n",
              "Oxy_Op4            0\n",
              "Oxy_Op5            0\n",
              "Oxy_Op6            0\n",
              "Oxy_Op7            0\n",
              "Oxy_Op8            0\n",
              "Oxy_Op9            0\n",
              "Oxy_Op10           0\n",
              "Oxy_Op11           0\n",
              "Oxy_Op12           0\n",
              "Oxy_Op13           0\n",
              "Oxy_Op14           0\n",
              "Oxy_Op15           0\n",
              "Oxy_Op16           0\n",
              "measurement        0\n",
              "ResponseCode    4064\n",
              "TypeCode         484\n",
              "AgeCode            0\n",
              "SexCode            0\n",
              "Name: 0, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1mChDpkbyx7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "2e4b0161-0e34-4e15-86ad-ba1c305417bd"
      },
      "source": [
        "#filling nan's (outlier çıkarıyor ortalama alırken)\n",
        "def filling_val(array):\n",
        "  temp = np.array(array)\n",
        "  temp2 = [x for x in temp if np.isnan(x) == False]\n",
        "  z = np.abs(stats.zscore(temp2))\n",
        "  ind1 = np.where(z > 3)\n",
        "  ind2 = ind1[0].tolist()\n",
        "  orj = temp2.copy()\n",
        "  for i in ind2:\n",
        "    k = temp2[i]\n",
        "    orj.remove(k)\n",
        "  return np.mean(orj)\n",
        "\n",
        "pre = data.copy()\n",
        "\n",
        "for col in pre.iloc[:,2:66].columns:\n",
        "  pre[col].fillna(filling_val(pre[col]), inplace=True)\n",
        "\n",
        "#outlier removing\n",
        "pre2 = pre.dropna(axis=0).reset_index()\n",
        "sel_index = pre2.iloc[:,0]\n",
        "clean = pre2.iloc[:,3:67].copy()\n",
        "clean2=clean[(np.abs(stats.zscore(clean)) <= 3).all(axis=1)].reset_index()\n",
        "sel_index2 = clean2.iloc[:,0]\n",
        "clean2"
      ],
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Hbo_Op1</th>\n",
              "      <th>Hbo_Op2</th>\n",
              "      <th>Hbo_Op3</th>\n",
              "      <th>Hbo_Op4</th>\n",
              "      <th>Hbo_Op5</th>\n",
              "      <th>Hbo_Op6</th>\n",
              "      <th>Hbo_Op7</th>\n",
              "      <th>Hbo_Op8</th>\n",
              "      <th>Hbo_Op9</th>\n",
              "      <th>Hbo_Op10</th>\n",
              "      <th>Hbo_Op11</th>\n",
              "      <th>Hbo_Op12</th>\n",
              "      <th>Hbo_Op13</th>\n",
              "      <th>Hbo_Op14</th>\n",
              "      <th>Hbo_Op15</th>\n",
              "      <th>Hbo_Op16</th>\n",
              "      <th>Hbr_Op1</th>\n",
              "      <th>Hbr_Op2</th>\n",
              "      <th>Hbr_Op3</th>\n",
              "      <th>Hbr_Op4</th>\n",
              "      <th>Hbr_Op5</th>\n",
              "      <th>Hbr_Op6</th>\n",
              "      <th>Hbr_Op7</th>\n",
              "      <th>Hbr_Op8</th>\n",
              "      <th>Hbr_Op9</th>\n",
              "      <th>Hbr_Op10</th>\n",
              "      <th>Hbr_Op11</th>\n",
              "      <th>Hbr_Op12</th>\n",
              "      <th>Hbr_Op13</th>\n",
              "      <th>Hbr_Op14</th>\n",
              "      <th>Hbr_Op15</th>\n",
              "      <th>Hbr_Op16</th>\n",
              "      <th>Hbt_Op1</th>\n",
              "      <th>Hbt_Op2</th>\n",
              "      <th>Hbt_Op3</th>\n",
              "      <th>Hbt_Op4</th>\n",
              "      <th>Hbt_Op5</th>\n",
              "      <th>Hbt_Op6</th>\n",
              "      <th>Hbt_Op7</th>\n",
              "      <th>Hbt_Op8</th>\n",
              "      <th>Hbt_Op9</th>\n",
              "      <th>Hbt_Op10</th>\n",
              "      <th>Hbt_Op11</th>\n",
              "      <th>Hbt_Op12</th>\n",
              "      <th>Hbt_Op13</th>\n",
              "      <th>Hbt_Op14</th>\n",
              "      <th>Hbt_Op15</th>\n",
              "      <th>Hbt_Op16</th>\n",
              "      <th>Oxy_Op1</th>\n",
              "      <th>Oxy_Op2</th>\n",
              "      <th>Oxy_Op3</th>\n",
              "      <th>Oxy_Op4</th>\n",
              "      <th>Oxy_Op5</th>\n",
              "      <th>Oxy_Op6</th>\n",
              "      <th>Oxy_Op7</th>\n",
              "      <th>Oxy_Op8</th>\n",
              "      <th>Oxy_Op9</th>\n",
              "      <th>Oxy_Op10</th>\n",
              "      <th>Oxy_Op11</th>\n",
              "      <th>Oxy_Op12</th>\n",
              "      <th>Oxy_Op13</th>\n",
              "      <th>Oxy_Op14</th>\n",
              "      <th>Oxy_Op15</th>\n",
              "      <th>Oxy_Op16</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.024675</td>\n",
              "      <td>-0.015218</td>\n",
              "      <td>0.032464</td>\n",
              "      <td>0.058835</td>\n",
              "      <td>0.005543</td>\n",
              "      <td>0.012666</td>\n",
              "      <td>0.021487</td>\n",
              "      <td>0.034923</td>\n",
              "      <td>0.022738</td>\n",
              "      <td>0.036211</td>\n",
              "      <td>0.008657</td>\n",
              "      <td>0.014996</td>\n",
              "      <td>-0.015077</td>\n",
              "      <td>0.002910</td>\n",
              "      <td>0.00153</td>\n",
              "      <td>-0.002817</td>\n",
              "      <td>0.045010</td>\n",
              "      <td>0.001022</td>\n",
              "      <td>0.052551</td>\n",
              "      <td>0.035209</td>\n",
              "      <td>0.013954</td>\n",
              "      <td>-0.013428</td>\n",
              "      <td>0.017544</td>\n",
              "      <td>-0.023915</td>\n",
              "      <td>0.014784</td>\n",
              "      <td>-0.024153</td>\n",
              "      <td>0.014272</td>\n",
              "      <td>-0.014024</td>\n",
              "      <td>0.014533</td>\n",
              "      <td>0.052528</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>-0.017059</td>\n",
              "      <td>0.069685</td>\n",
              "      <td>-0.014196</td>\n",
              "      <td>0.085015</td>\n",
              "      <td>0.094044</td>\n",
              "      <td>0.024470</td>\n",
              "      <td>-0.000973</td>\n",
              "      <td>0.041987</td>\n",
              "      <td>0.013063</td>\n",
              "      <td>0.041198</td>\n",
              "      <td>0.011177</td>\n",
              "      <td>0.023116</td>\n",
              "      <td>-0.001481</td>\n",
              "      <td>0.003744</td>\n",
              "      <td>0.055438</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>-0.018717</td>\n",
              "      <td>-0.020335</td>\n",
              "      <td>-0.016239</td>\n",
              "      <td>-0.020087</td>\n",
              "      <td>0.023625</td>\n",
              "      <td>-0.009547</td>\n",
              "      <td>0.027409</td>\n",
              "      <td>0.003903</td>\n",
              "      <td>0.058095</td>\n",
              "      <td>0.005639</td>\n",
              "      <td>0.056720</td>\n",
              "      <td>-0.004408</td>\n",
              "      <td>0.029645</td>\n",
              "      <td>-0.030096</td>\n",
              "      <td>-0.049618</td>\n",
              "      <td>-0.000364</td>\n",
              "      <td>0.014018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.030511</td>\n",
              "      <td>-0.012349</td>\n",
              "      <td>0.039233</td>\n",
              "      <td>0.074926</td>\n",
              "      <td>0.005543</td>\n",
              "      <td>0.012666</td>\n",
              "      <td>0.021487</td>\n",
              "      <td>0.034923</td>\n",
              "      <td>0.022738</td>\n",
              "      <td>0.036211</td>\n",
              "      <td>0.008657</td>\n",
              "      <td>0.014996</td>\n",
              "      <td>-0.015077</td>\n",
              "      <td>-0.003348</td>\n",
              "      <td>0.00153</td>\n",
              "      <td>-0.002817</td>\n",
              "      <td>0.053890</td>\n",
              "      <td>0.000589</td>\n",
              "      <td>0.060653</td>\n",
              "      <td>0.040470</td>\n",
              "      <td>0.013954</td>\n",
              "      <td>-0.013428</td>\n",
              "      <td>0.017544</td>\n",
              "      <td>-0.023915</td>\n",
              "      <td>0.014784</td>\n",
              "      <td>-0.024153</td>\n",
              "      <td>0.014272</td>\n",
              "      <td>-0.014024</td>\n",
              "      <td>0.014533</td>\n",
              "      <td>0.075030</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>-0.017059</td>\n",
              "      <td>0.084401</td>\n",
              "      <td>-0.011760</td>\n",
              "      <td>0.099886</td>\n",
              "      <td>0.115395</td>\n",
              "      <td>0.024470</td>\n",
              "      <td>-0.000973</td>\n",
              "      <td>0.041987</td>\n",
              "      <td>0.013063</td>\n",
              "      <td>0.041198</td>\n",
              "      <td>0.011177</td>\n",
              "      <td>0.023116</td>\n",
              "      <td>-0.001481</td>\n",
              "      <td>0.003744</td>\n",
              "      <td>0.071682</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>-0.018717</td>\n",
              "      <td>-0.023379</td>\n",
              "      <td>-0.012938</td>\n",
              "      <td>-0.021419</td>\n",
              "      <td>0.034456</td>\n",
              "      <td>-0.009547</td>\n",
              "      <td>0.027409</td>\n",
              "      <td>0.003903</td>\n",
              "      <td>0.058095</td>\n",
              "      <td>0.005639</td>\n",
              "      <td>0.056720</td>\n",
              "      <td>-0.004408</td>\n",
              "      <td>0.029645</td>\n",
              "      <td>-0.030096</td>\n",
              "      <td>-0.078378</td>\n",
              "      <td>-0.000364</td>\n",
              "      <td>0.014018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.039081</td>\n",
              "      <td>-0.004290</td>\n",
              "      <td>0.048589</td>\n",
              "      <td>0.094117</td>\n",
              "      <td>0.005543</td>\n",
              "      <td>0.012666</td>\n",
              "      <td>0.021487</td>\n",
              "      <td>0.034923</td>\n",
              "      <td>0.022738</td>\n",
              "      <td>0.036211</td>\n",
              "      <td>0.008657</td>\n",
              "      <td>0.014996</td>\n",
              "      <td>-0.015077</td>\n",
              "      <td>-0.003982</td>\n",
              "      <td>0.00153</td>\n",
              "      <td>-0.002817</td>\n",
              "      <td>0.058895</td>\n",
              "      <td>-0.002358</td>\n",
              "      <td>0.063509</td>\n",
              "      <td>0.041851</td>\n",
              "      <td>0.013954</td>\n",
              "      <td>-0.013428</td>\n",
              "      <td>0.017544</td>\n",
              "      <td>-0.023915</td>\n",
              "      <td>0.014784</td>\n",
              "      <td>-0.024153</td>\n",
              "      <td>0.014272</td>\n",
              "      <td>-0.014024</td>\n",
              "      <td>0.014533</td>\n",
              "      <td>0.090856</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>-0.017059</td>\n",
              "      <td>0.097976</td>\n",
              "      <td>-0.006647</td>\n",
              "      <td>0.112098</td>\n",
              "      <td>0.135968</td>\n",
              "      <td>0.024470</td>\n",
              "      <td>-0.000973</td>\n",
              "      <td>0.041987</td>\n",
              "      <td>0.013063</td>\n",
              "      <td>0.041198</td>\n",
              "      <td>0.011177</td>\n",
              "      <td>0.023116</td>\n",
              "      <td>-0.001481</td>\n",
              "      <td>0.003744</td>\n",
              "      <td>0.086874</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>-0.018717</td>\n",
              "      <td>-0.019813</td>\n",
              "      <td>-0.001932</td>\n",
              "      <td>-0.014920</td>\n",
              "      <td>0.052266</td>\n",
              "      <td>-0.009547</td>\n",
              "      <td>0.027409</td>\n",
              "      <td>0.003903</td>\n",
              "      <td>0.058095</td>\n",
              "      <td>0.005639</td>\n",
              "      <td>0.056720</td>\n",
              "      <td>-0.004408</td>\n",
              "      <td>0.029645</td>\n",
              "      <td>-0.030096</td>\n",
              "      <td>-0.094838</td>\n",
              "      <td>-0.000364</td>\n",
              "      <td>0.014018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.050448</td>\n",
              "      <td>0.008479</td>\n",
              "      <td>0.060856</td>\n",
              "      <td>0.115959</td>\n",
              "      <td>0.005543</td>\n",
              "      <td>0.012666</td>\n",
              "      <td>0.021487</td>\n",
              "      <td>0.034923</td>\n",
              "      <td>0.022738</td>\n",
              "      <td>0.036211</td>\n",
              "      <td>0.008657</td>\n",
              "      <td>0.014996</td>\n",
              "      <td>-0.015077</td>\n",
              "      <td>0.003388</td>\n",
              "      <td>0.00153</td>\n",
              "      <td>-0.002817</td>\n",
              "      <td>0.059176</td>\n",
              "      <td>-0.008715</td>\n",
              "      <td>0.060312</td>\n",
              "      <td>0.038532</td>\n",
              "      <td>0.013954</td>\n",
              "      <td>-0.013428</td>\n",
              "      <td>0.017544</td>\n",
              "      <td>-0.023915</td>\n",
              "      <td>0.014784</td>\n",
              "      <td>-0.024153</td>\n",
              "      <td>0.014272</td>\n",
              "      <td>-0.014024</td>\n",
              "      <td>0.014533</td>\n",
              "      <td>0.095729</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>-0.017059</td>\n",
              "      <td>0.109625</td>\n",
              "      <td>-0.000236</td>\n",
              "      <td>0.121168</td>\n",
              "      <td>0.154490</td>\n",
              "      <td>0.024470</td>\n",
              "      <td>-0.000973</td>\n",
              "      <td>0.041987</td>\n",
              "      <td>0.013063</td>\n",
              "      <td>0.041198</td>\n",
              "      <td>0.011177</td>\n",
              "      <td>0.023116</td>\n",
              "      <td>-0.001481</td>\n",
              "      <td>0.003744</td>\n",
              "      <td>0.099117</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>-0.018717</td>\n",
              "      <td>-0.008728</td>\n",
              "      <td>0.017194</td>\n",
              "      <td>0.000544</td>\n",
              "      <td>0.077427</td>\n",
              "      <td>-0.009547</td>\n",
              "      <td>0.027409</td>\n",
              "      <td>0.003903</td>\n",
              "      <td>0.058095</td>\n",
              "      <td>0.005639</td>\n",
              "      <td>0.056720</td>\n",
              "      <td>-0.004408</td>\n",
              "      <td>0.029645</td>\n",
              "      <td>-0.030096</td>\n",
              "      <td>-0.092341</td>\n",
              "      <td>-0.000364</td>\n",
              "      <td>0.014018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.064245</td>\n",
              "      <td>0.024718</td>\n",
              "      <td>0.075855</td>\n",
              "      <td>0.139330</td>\n",
              "      <td>0.005543</td>\n",
              "      <td>0.012666</td>\n",
              "      <td>0.021487</td>\n",
              "      <td>0.034923</td>\n",
              "      <td>0.022738</td>\n",
              "      <td>0.036211</td>\n",
              "      <td>0.008657</td>\n",
              "      <td>0.014996</td>\n",
              "      <td>-0.015077</td>\n",
              "      <td>0.019458</td>\n",
              "      <td>0.00153</td>\n",
              "      <td>-0.002817</td>\n",
              "      <td>0.054452</td>\n",
              "      <td>-0.018922</td>\n",
              "      <td>0.051060</td>\n",
              "      <td>0.030282</td>\n",
              "      <td>0.013954</td>\n",
              "      <td>-0.013428</td>\n",
              "      <td>0.017544</td>\n",
              "      <td>-0.023915</td>\n",
              "      <td>0.014784</td>\n",
              "      <td>-0.024153</td>\n",
              "      <td>0.014272</td>\n",
              "      <td>-0.014024</td>\n",
              "      <td>0.014533</td>\n",
              "      <td>0.087221</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>-0.017059</td>\n",
              "      <td>0.118697</td>\n",
              "      <td>0.005796</td>\n",
              "      <td>0.126915</td>\n",
              "      <td>0.169612</td>\n",
              "      <td>0.024470</td>\n",
              "      <td>-0.000973</td>\n",
              "      <td>0.041987</td>\n",
              "      <td>0.013063</td>\n",
              "      <td>0.041198</td>\n",
              "      <td>0.011177</td>\n",
              "      <td>0.023116</td>\n",
              "      <td>-0.001481</td>\n",
              "      <td>0.003744</td>\n",
              "      <td>0.106679</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>-0.018717</td>\n",
              "      <td>0.009794</td>\n",
              "      <td>0.043639</td>\n",
              "      <td>0.024794</td>\n",
              "      <td>0.109047</td>\n",
              "      <td>-0.009547</td>\n",
              "      <td>0.027409</td>\n",
              "      <td>0.003903</td>\n",
              "      <td>0.058095</td>\n",
              "      <td>0.005639</td>\n",
              "      <td>0.056720</td>\n",
              "      <td>-0.004408</td>\n",
              "      <td>0.029645</td>\n",
              "      <td>-0.030096</td>\n",
              "      <td>-0.067763</td>\n",
              "      <td>-0.000364</td>\n",
              "      <td>0.014018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60003</th>\n",
              "      <td>77670</td>\n",
              "      <td>0.034821</td>\n",
              "      <td>-0.115473</td>\n",
              "      <td>0.171372</td>\n",
              "      <td>0.236536</td>\n",
              "      <td>-0.010133</td>\n",
              "      <td>0.203421</td>\n",
              "      <td>-0.008541</td>\n",
              "      <td>0.075152</td>\n",
              "      <td>0.095367</td>\n",
              "      <td>0.170726</td>\n",
              "      <td>-0.008445</td>\n",
              "      <td>0.129385</td>\n",
              "      <td>-0.171005</td>\n",
              "      <td>0.020026</td>\n",
              "      <td>0.00153</td>\n",
              "      <td>-0.530636</td>\n",
              "      <td>-0.138584</td>\n",
              "      <td>0.035375</td>\n",
              "      <td>0.002690</td>\n",
              "      <td>-0.035139</td>\n",
              "      <td>0.064553</td>\n",
              "      <td>-0.140066</td>\n",
              "      <td>0.012067</td>\n",
              "      <td>-0.057594</td>\n",
              "      <td>-0.238289</td>\n",
              "      <td>-0.119395</td>\n",
              "      <td>-0.178760</td>\n",
              "      <td>-0.157317</td>\n",
              "      <td>-0.012186</td>\n",
              "      <td>-0.101482</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>0.395693</td>\n",
              "      <td>-0.103764</td>\n",
              "      <td>-0.080098</td>\n",
              "      <td>0.174061</td>\n",
              "      <td>0.201397</td>\n",
              "      <td>0.054420</td>\n",
              "      <td>0.063355</td>\n",
              "      <td>0.003526</td>\n",
              "      <td>0.017558</td>\n",
              "      <td>-0.142922</td>\n",
              "      <td>0.051331</td>\n",
              "      <td>-0.187205</td>\n",
              "      <td>-0.027932</td>\n",
              "      <td>-0.183190</td>\n",
              "      <td>-0.081456</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>-0.134942</td>\n",
              "      <td>0.173405</td>\n",
              "      <td>-0.150848</td>\n",
              "      <td>0.168682</td>\n",
              "      <td>0.271675</td>\n",
              "      <td>-0.074685</td>\n",
              "      <td>0.343487</td>\n",
              "      <td>-0.020608</td>\n",
              "      <td>0.132747</td>\n",
              "      <td>0.333656</td>\n",
              "      <td>0.290121</td>\n",
              "      <td>0.170315</td>\n",
              "      <td>0.286703</td>\n",
              "      <td>-0.158819</td>\n",
              "      <td>0.121507</td>\n",
              "      <td>-0.000364</td>\n",
              "      <td>-0.926329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60004</th>\n",
              "      <td>77671</td>\n",
              "      <td>0.047532</td>\n",
              "      <td>-0.110554</td>\n",
              "      <td>0.196474</td>\n",
              "      <td>0.261890</td>\n",
              "      <td>-0.012670</td>\n",
              "      <td>0.204857</td>\n",
              "      <td>-0.011992</td>\n",
              "      <td>0.082482</td>\n",
              "      <td>0.070932</td>\n",
              "      <td>0.189418</td>\n",
              "      <td>-0.012588</td>\n",
              "      <td>0.154542</td>\n",
              "      <td>-0.182753</td>\n",
              "      <td>0.028790</td>\n",
              "      <td>0.00153</td>\n",
              "      <td>-0.558346</td>\n",
              "      <td>-0.152954</td>\n",
              "      <td>0.033928</td>\n",
              "      <td>-0.021223</td>\n",
              "      <td>-0.044146</td>\n",
              "      <td>0.054283</td>\n",
              "      <td>-0.150759</td>\n",
              "      <td>-0.013154</td>\n",
              "      <td>-0.075916</td>\n",
              "      <td>-0.301802</td>\n",
              "      <td>-0.122252</td>\n",
              "      <td>-0.191145</td>\n",
              "      <td>-0.166947</td>\n",
              "      <td>0.003031</td>\n",
              "      <td>-0.099811</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>0.439347</td>\n",
              "      <td>-0.105422</td>\n",
              "      <td>-0.076626</td>\n",
              "      <td>0.175251</td>\n",
              "      <td>0.217744</td>\n",
              "      <td>0.041613</td>\n",
              "      <td>0.054098</td>\n",
              "      <td>-0.025146</td>\n",
              "      <td>0.006566</td>\n",
              "      <td>-0.230870</td>\n",
              "      <td>0.067166</td>\n",
              "      <td>-0.203734</td>\n",
              "      <td>-0.012405</td>\n",
              "      <td>-0.179722</td>\n",
              "      <td>-0.071021</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>-0.118999</td>\n",
              "      <td>0.200487</td>\n",
              "      <td>-0.144483</td>\n",
              "      <td>0.217698</td>\n",
              "      <td>0.306036</td>\n",
              "      <td>-0.066953</td>\n",
              "      <td>0.355617</td>\n",
              "      <td>0.001163</td>\n",
              "      <td>0.158399</td>\n",
              "      <td>0.372733</td>\n",
              "      <td>0.311670</td>\n",
              "      <td>0.178557</td>\n",
              "      <td>0.321489</td>\n",
              "      <td>-0.185784</td>\n",
              "      <td>0.128601</td>\n",
              "      <td>-0.000364</td>\n",
              "      <td>-0.997693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60005</th>\n",
              "      <td>77672</td>\n",
              "      <td>0.057649</td>\n",
              "      <td>-0.105592</td>\n",
              "      <td>0.220227</td>\n",
              "      <td>0.284594</td>\n",
              "      <td>-0.012182</td>\n",
              "      <td>0.206823</td>\n",
              "      <td>-0.013887</td>\n",
              "      <td>0.091615</td>\n",
              "      <td>0.048774</td>\n",
              "      <td>0.215278</td>\n",
              "      <td>-0.010999</td>\n",
              "      <td>0.185102</td>\n",
              "      <td>-0.174802</td>\n",
              "      <td>0.059567</td>\n",
              "      <td>0.00153</td>\n",
              "      <td>-0.568724</td>\n",
              "      <td>-0.163082</td>\n",
              "      <td>0.036482</td>\n",
              "      <td>-0.043304</td>\n",
              "      <td>-0.049219</td>\n",
              "      <td>0.040164</td>\n",
              "      <td>-0.161945</td>\n",
              "      <td>-0.042853</td>\n",
              "      <td>-0.096645</td>\n",
              "      <td>-0.377955</td>\n",
              "      <td>-0.127309</td>\n",
              "      <td>-0.205245</td>\n",
              "      <td>-0.177073</td>\n",
              "      <td>0.007069</td>\n",
              "      <td>-0.111383</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>0.475242</td>\n",
              "      <td>-0.105433</td>\n",
              "      <td>-0.069110</td>\n",
              "      <td>0.176924</td>\n",
              "      <td>0.235375</td>\n",
              "      <td>0.027982</td>\n",
              "      <td>0.044878</td>\n",
              "      <td>-0.056740</td>\n",
              "      <td>-0.005030</td>\n",
              "      <td>-0.329180</td>\n",
              "      <td>0.087970</td>\n",
              "      <td>-0.216244</td>\n",
              "      <td>0.008028</td>\n",
              "      <td>-0.167733</td>\n",
              "      <td>-0.051815</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>-0.093483</td>\n",
              "      <td>0.220732</td>\n",
              "      <td>-0.142074</td>\n",
              "      <td>0.263531</td>\n",
              "      <td>0.333812</td>\n",
              "      <td>-0.052346</td>\n",
              "      <td>0.368768</td>\n",
              "      <td>0.028965</td>\n",
              "      <td>0.188260</td>\n",
              "      <td>0.426729</td>\n",
              "      <td>0.342587</td>\n",
              "      <td>0.194246</td>\n",
              "      <td>0.362175</td>\n",
              "      <td>-0.181871</td>\n",
              "      <td>0.170950</td>\n",
              "      <td>-0.000364</td>\n",
              "      <td>-1.043966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60006</th>\n",
              "      <td>77673</td>\n",
              "      <td>0.066079</td>\n",
              "      <td>-0.099071</td>\n",
              "      <td>0.244751</td>\n",
              "      <td>0.306901</td>\n",
              "      <td>-0.008598</td>\n",
              "      <td>0.210741</td>\n",
              "      <td>-0.014197</td>\n",
              "      <td>0.102570</td>\n",
              "      <td>0.030423</td>\n",
              "      <td>0.249247</td>\n",
              "      <td>-0.002708</td>\n",
              "      <td>0.221406</td>\n",
              "      <td>-0.149129</td>\n",
              "      <td>0.109211</td>\n",
              "      <td>0.00153</td>\n",
              "      <td>-0.563941</td>\n",
              "      <td>-0.169730</td>\n",
              "      <td>0.042060</td>\n",
              "      <td>-0.065157</td>\n",
              "      <td>-0.052584</td>\n",
              "      <td>0.022522</td>\n",
              "      <td>-0.175324</td>\n",
              "      <td>-0.076534</td>\n",
              "      <td>-0.119258</td>\n",
              "      <td>-0.469808</td>\n",
              "      <td>-0.136817</td>\n",
              "      <td>-0.223019</td>\n",
              "      <td>-0.189165</td>\n",
              "      <td>-0.001855</td>\n",
              "      <td>-0.137473</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>0.502330</td>\n",
              "      <td>-0.103651</td>\n",
              "      <td>-0.057010</td>\n",
              "      <td>0.179594</td>\n",
              "      <td>0.254318</td>\n",
              "      <td>0.013924</td>\n",
              "      <td>0.035418</td>\n",
              "      <td>-0.090732</td>\n",
              "      <td>-0.016689</td>\n",
              "      <td>-0.439385</td>\n",
              "      <td>0.112429</td>\n",
              "      <td>-0.225728</td>\n",
              "      <td>0.032242</td>\n",
              "      <td>-0.150984</td>\n",
              "      <td>-0.028263</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>-0.061611</td>\n",
              "      <td>0.235810</td>\n",
              "      <td>-0.141131</td>\n",
              "      <td>0.309909</td>\n",
              "      <td>0.359485</td>\n",
              "      <td>-0.031121</td>\n",
              "      <td>0.386065</td>\n",
              "      <td>0.062337</td>\n",
              "      <td>0.221828</td>\n",
              "      <td>0.500230</td>\n",
              "      <td>0.386064</td>\n",
              "      <td>0.220311</td>\n",
              "      <td>0.410571</td>\n",
              "      <td>-0.147274</td>\n",
              "      <td>0.246684</td>\n",
              "      <td>-0.000364</td>\n",
              "      <td>-1.066270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60007</th>\n",
              "      <td>77674</td>\n",
              "      <td>0.074707</td>\n",
              "      <td>-0.088321</td>\n",
              "      <td>0.273382</td>\n",
              "      <td>0.332519</td>\n",
              "      <td>-0.001164</td>\n",
              "      <td>0.219474</td>\n",
              "      <td>-0.011959</td>\n",
              "      <td>0.116376</td>\n",
              "      <td>0.017980</td>\n",
              "      <td>0.293136</td>\n",
              "      <td>0.013737</td>\n",
              "      <td>0.264685</td>\n",
              "      <td>-0.108912</td>\n",
              "      <td>0.173457</td>\n",
              "      <td>0.00153</td>\n",
              "      <td>-0.546937</td>\n",
              "      <td>-0.174522</td>\n",
              "      <td>0.048707</td>\n",
              "      <td>-0.089601</td>\n",
              "      <td>-0.057659</td>\n",
              "      <td>0.001148</td>\n",
              "      <td>-0.193786</td>\n",
              "      <td>-0.114654</td>\n",
              "      <td>-0.144103</td>\n",
              "      <td>-0.580800</td>\n",
              "      <td>-0.154351</td>\n",
              "      <td>-0.247110</td>\n",
              "      <td>-0.205706</td>\n",
              "      <td>-0.024139</td>\n",
              "      <td>-0.177692</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>0.520364</td>\n",
              "      <td>-0.099815</td>\n",
              "      <td>-0.039613</td>\n",
              "      <td>0.183781</td>\n",
              "      <td>0.274860</td>\n",
              "      <td>-0.000015</td>\n",
              "      <td>0.025688</td>\n",
              "      <td>-0.126613</td>\n",
              "      <td>-0.027727</td>\n",
              "      <td>-0.562820</td>\n",
              "      <td>0.138785</td>\n",
              "      <td>-0.233372</td>\n",
              "      <td>0.058978</td>\n",
              "      <td>-0.133050</td>\n",
              "      <td>-0.004235</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>-0.026573</td>\n",
              "      <td>0.249229</td>\n",
              "      <td>-0.137028</td>\n",
              "      <td>0.362983</td>\n",
              "      <td>0.390177</td>\n",
              "      <td>-0.002312</td>\n",
              "      <td>0.413259</td>\n",
              "      <td>0.102695</td>\n",
              "      <td>0.260478</td>\n",
              "      <td>0.598779</td>\n",
              "      <td>0.447487</td>\n",
              "      <td>0.260847</td>\n",
              "      <td>0.470391</td>\n",
              "      <td>-0.084773</td>\n",
              "      <td>0.351150</td>\n",
              "      <td>-0.000364</td>\n",
              "      <td>-1.067302</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>60008 rows × 65 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       index   Hbo_Op1   Hbo_Op2  ...  Oxy_Op14  Oxy_Op15  Oxy_Op16\n",
              "0          0  0.024675 -0.015218  ... -0.049618 -0.000364  0.014018\n",
              "1          1  0.030511 -0.012349  ... -0.078378 -0.000364  0.014018\n",
              "2          2  0.039081 -0.004290  ... -0.094838 -0.000364  0.014018\n",
              "3          3  0.050448  0.008479  ... -0.092341 -0.000364  0.014018\n",
              "4          4  0.064245  0.024718  ... -0.067763 -0.000364  0.014018\n",
              "...      ...       ...       ...  ...       ...       ...       ...\n",
              "60003  77670  0.034821 -0.115473  ...  0.121507 -0.000364 -0.926329\n",
              "60004  77671  0.047532 -0.110554  ...  0.128601 -0.000364 -0.997693\n",
              "60005  77672  0.057649 -0.105592  ...  0.170950 -0.000364 -1.043966\n",
              "60006  77673  0.066079 -0.099071  ...  0.246684 -0.000364 -1.066270\n",
              "60007  77674  0.074707 -0.088321  ...  0.351150 -0.000364 -1.067302\n",
              "\n",
              "[60008 rows x 65 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5CcujZmMQCR",
        "colab_type": "text"
      },
      "source": [
        "### Deneme 2 (Isolation Forest)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PENOqkHaIH4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "67dc7f23-d824-46a2-8ff1-97796aa33430"
      },
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "pre_clean =  MinMaxScaler().fit_transform(clean)\n",
        "clf_is = IsolationForest(max_samples=100, random_state=0)\n",
        "clf_is.fit(pre_clean)\n",
        "y_pred_is = clf_is.predict(pre_clean)\n",
        "clean3 = clean[np.where(y_pred_is == 1, True, False)].reset_index()\n",
        "clean3"
      ],
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Hbo_Op1</th>\n",
              "      <th>Hbo_Op2</th>\n",
              "      <th>Hbo_Op3</th>\n",
              "      <th>Hbo_Op4</th>\n",
              "      <th>Hbo_Op5</th>\n",
              "      <th>Hbo_Op6</th>\n",
              "      <th>Hbo_Op7</th>\n",
              "      <th>Hbo_Op8</th>\n",
              "      <th>Hbo_Op9</th>\n",
              "      <th>Hbo_Op10</th>\n",
              "      <th>Hbo_Op11</th>\n",
              "      <th>Hbo_Op12</th>\n",
              "      <th>Hbo_Op13</th>\n",
              "      <th>Hbo_Op14</th>\n",
              "      <th>Hbo_Op15</th>\n",
              "      <th>Hbo_Op16</th>\n",
              "      <th>Hbr_Op1</th>\n",
              "      <th>Hbr_Op2</th>\n",
              "      <th>Hbr_Op3</th>\n",
              "      <th>Hbr_Op4</th>\n",
              "      <th>Hbr_Op5</th>\n",
              "      <th>Hbr_Op6</th>\n",
              "      <th>Hbr_Op7</th>\n",
              "      <th>Hbr_Op8</th>\n",
              "      <th>Hbr_Op9</th>\n",
              "      <th>Hbr_Op10</th>\n",
              "      <th>Hbr_Op11</th>\n",
              "      <th>Hbr_Op12</th>\n",
              "      <th>Hbr_Op13</th>\n",
              "      <th>Hbr_Op14</th>\n",
              "      <th>Hbr_Op15</th>\n",
              "      <th>Hbr_Op16</th>\n",
              "      <th>Hbt_Op1</th>\n",
              "      <th>Hbt_Op2</th>\n",
              "      <th>Hbt_Op3</th>\n",
              "      <th>Hbt_Op4</th>\n",
              "      <th>Hbt_Op5</th>\n",
              "      <th>Hbt_Op6</th>\n",
              "      <th>Hbt_Op7</th>\n",
              "      <th>Hbt_Op8</th>\n",
              "      <th>Hbt_Op9</th>\n",
              "      <th>Hbt_Op10</th>\n",
              "      <th>Hbt_Op11</th>\n",
              "      <th>Hbt_Op12</th>\n",
              "      <th>Hbt_Op13</th>\n",
              "      <th>Hbt_Op14</th>\n",
              "      <th>Hbt_Op15</th>\n",
              "      <th>Hbt_Op16</th>\n",
              "      <th>Oxy_Op1</th>\n",
              "      <th>Oxy_Op2</th>\n",
              "      <th>Oxy_Op3</th>\n",
              "      <th>Oxy_Op4</th>\n",
              "      <th>Oxy_Op5</th>\n",
              "      <th>Oxy_Op6</th>\n",
              "      <th>Oxy_Op7</th>\n",
              "      <th>Oxy_Op8</th>\n",
              "      <th>Oxy_Op9</th>\n",
              "      <th>Oxy_Op10</th>\n",
              "      <th>Oxy_Op11</th>\n",
              "      <th>Oxy_Op12</th>\n",
              "      <th>Oxy_Op13</th>\n",
              "      <th>Oxy_Op14</th>\n",
              "      <th>Oxy_Op15</th>\n",
              "      <th>Oxy_Op16</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.024675</td>\n",
              "      <td>-0.015218</td>\n",
              "      <td>0.032464</td>\n",
              "      <td>0.058835</td>\n",
              "      <td>0.005543</td>\n",
              "      <td>0.012666</td>\n",
              "      <td>0.021487</td>\n",
              "      <td>0.034923</td>\n",
              "      <td>0.022738</td>\n",
              "      <td>0.036211</td>\n",
              "      <td>0.008657</td>\n",
              "      <td>0.014996</td>\n",
              "      <td>-0.015077</td>\n",
              "      <td>0.002910</td>\n",
              "      <td>0.00153</td>\n",
              "      <td>-0.002817</td>\n",
              "      <td>0.045010</td>\n",
              "      <td>0.001022</td>\n",
              "      <td>0.052551</td>\n",
              "      <td>0.035209</td>\n",
              "      <td>0.013954</td>\n",
              "      <td>-0.013428</td>\n",
              "      <td>0.017544</td>\n",
              "      <td>-0.023915</td>\n",
              "      <td>0.014784</td>\n",
              "      <td>-0.024153</td>\n",
              "      <td>0.014272</td>\n",
              "      <td>-0.014024</td>\n",
              "      <td>0.014533</td>\n",
              "      <td>0.052528</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>-0.017059</td>\n",
              "      <td>0.069685</td>\n",
              "      <td>-0.014196</td>\n",
              "      <td>0.085015</td>\n",
              "      <td>0.094044</td>\n",
              "      <td>0.024470</td>\n",
              "      <td>-0.000973</td>\n",
              "      <td>0.041987</td>\n",
              "      <td>0.013063</td>\n",
              "      <td>0.041198</td>\n",
              "      <td>0.011177</td>\n",
              "      <td>0.023116</td>\n",
              "      <td>-0.001481</td>\n",
              "      <td>0.003744</td>\n",
              "      <td>0.055438</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>-0.018717</td>\n",
              "      <td>-0.020335</td>\n",
              "      <td>-0.016239</td>\n",
              "      <td>-0.020087</td>\n",
              "      <td>0.023625</td>\n",
              "      <td>-0.009547</td>\n",
              "      <td>0.027409</td>\n",
              "      <td>0.003903</td>\n",
              "      <td>0.058095</td>\n",
              "      <td>0.005639</td>\n",
              "      <td>0.056720</td>\n",
              "      <td>-0.004408</td>\n",
              "      <td>0.029645</td>\n",
              "      <td>-0.030096</td>\n",
              "      <td>-0.049618</td>\n",
              "      <td>-0.000364</td>\n",
              "      <td>0.014018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.030511</td>\n",
              "      <td>-0.012349</td>\n",
              "      <td>0.039233</td>\n",
              "      <td>0.074926</td>\n",
              "      <td>0.005543</td>\n",
              "      <td>0.012666</td>\n",
              "      <td>0.021487</td>\n",
              "      <td>0.034923</td>\n",
              "      <td>0.022738</td>\n",
              "      <td>0.036211</td>\n",
              "      <td>0.008657</td>\n",
              "      <td>0.014996</td>\n",
              "      <td>-0.015077</td>\n",
              "      <td>-0.003348</td>\n",
              "      <td>0.00153</td>\n",
              "      <td>-0.002817</td>\n",
              "      <td>0.053890</td>\n",
              "      <td>0.000589</td>\n",
              "      <td>0.060653</td>\n",
              "      <td>0.040470</td>\n",
              "      <td>0.013954</td>\n",
              "      <td>-0.013428</td>\n",
              "      <td>0.017544</td>\n",
              "      <td>-0.023915</td>\n",
              "      <td>0.014784</td>\n",
              "      <td>-0.024153</td>\n",
              "      <td>0.014272</td>\n",
              "      <td>-0.014024</td>\n",
              "      <td>0.014533</td>\n",
              "      <td>0.075030</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>-0.017059</td>\n",
              "      <td>0.084401</td>\n",
              "      <td>-0.011760</td>\n",
              "      <td>0.099886</td>\n",
              "      <td>0.115395</td>\n",
              "      <td>0.024470</td>\n",
              "      <td>-0.000973</td>\n",
              "      <td>0.041987</td>\n",
              "      <td>0.013063</td>\n",
              "      <td>0.041198</td>\n",
              "      <td>0.011177</td>\n",
              "      <td>0.023116</td>\n",
              "      <td>-0.001481</td>\n",
              "      <td>0.003744</td>\n",
              "      <td>0.071682</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>-0.018717</td>\n",
              "      <td>-0.023379</td>\n",
              "      <td>-0.012938</td>\n",
              "      <td>-0.021419</td>\n",
              "      <td>0.034456</td>\n",
              "      <td>-0.009547</td>\n",
              "      <td>0.027409</td>\n",
              "      <td>0.003903</td>\n",
              "      <td>0.058095</td>\n",
              "      <td>0.005639</td>\n",
              "      <td>0.056720</td>\n",
              "      <td>-0.004408</td>\n",
              "      <td>0.029645</td>\n",
              "      <td>-0.030096</td>\n",
              "      <td>-0.078378</td>\n",
              "      <td>-0.000364</td>\n",
              "      <td>0.014018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.039081</td>\n",
              "      <td>-0.004290</td>\n",
              "      <td>0.048589</td>\n",
              "      <td>0.094117</td>\n",
              "      <td>0.005543</td>\n",
              "      <td>0.012666</td>\n",
              "      <td>0.021487</td>\n",
              "      <td>0.034923</td>\n",
              "      <td>0.022738</td>\n",
              "      <td>0.036211</td>\n",
              "      <td>0.008657</td>\n",
              "      <td>0.014996</td>\n",
              "      <td>-0.015077</td>\n",
              "      <td>-0.003982</td>\n",
              "      <td>0.00153</td>\n",
              "      <td>-0.002817</td>\n",
              "      <td>0.058895</td>\n",
              "      <td>-0.002358</td>\n",
              "      <td>0.063509</td>\n",
              "      <td>0.041851</td>\n",
              "      <td>0.013954</td>\n",
              "      <td>-0.013428</td>\n",
              "      <td>0.017544</td>\n",
              "      <td>-0.023915</td>\n",
              "      <td>0.014784</td>\n",
              "      <td>-0.024153</td>\n",
              "      <td>0.014272</td>\n",
              "      <td>-0.014024</td>\n",
              "      <td>0.014533</td>\n",
              "      <td>0.090856</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>-0.017059</td>\n",
              "      <td>0.097976</td>\n",
              "      <td>-0.006647</td>\n",
              "      <td>0.112098</td>\n",
              "      <td>0.135968</td>\n",
              "      <td>0.024470</td>\n",
              "      <td>-0.000973</td>\n",
              "      <td>0.041987</td>\n",
              "      <td>0.013063</td>\n",
              "      <td>0.041198</td>\n",
              "      <td>0.011177</td>\n",
              "      <td>0.023116</td>\n",
              "      <td>-0.001481</td>\n",
              "      <td>0.003744</td>\n",
              "      <td>0.086874</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>-0.018717</td>\n",
              "      <td>-0.019813</td>\n",
              "      <td>-0.001932</td>\n",
              "      <td>-0.014920</td>\n",
              "      <td>0.052266</td>\n",
              "      <td>-0.009547</td>\n",
              "      <td>0.027409</td>\n",
              "      <td>0.003903</td>\n",
              "      <td>0.058095</td>\n",
              "      <td>0.005639</td>\n",
              "      <td>0.056720</td>\n",
              "      <td>-0.004408</td>\n",
              "      <td>0.029645</td>\n",
              "      <td>-0.030096</td>\n",
              "      <td>-0.094838</td>\n",
              "      <td>-0.000364</td>\n",
              "      <td>0.014018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.050448</td>\n",
              "      <td>0.008479</td>\n",
              "      <td>0.060856</td>\n",
              "      <td>0.115959</td>\n",
              "      <td>0.005543</td>\n",
              "      <td>0.012666</td>\n",
              "      <td>0.021487</td>\n",
              "      <td>0.034923</td>\n",
              "      <td>0.022738</td>\n",
              "      <td>0.036211</td>\n",
              "      <td>0.008657</td>\n",
              "      <td>0.014996</td>\n",
              "      <td>-0.015077</td>\n",
              "      <td>0.003388</td>\n",
              "      <td>0.00153</td>\n",
              "      <td>-0.002817</td>\n",
              "      <td>0.059176</td>\n",
              "      <td>-0.008715</td>\n",
              "      <td>0.060312</td>\n",
              "      <td>0.038532</td>\n",
              "      <td>0.013954</td>\n",
              "      <td>-0.013428</td>\n",
              "      <td>0.017544</td>\n",
              "      <td>-0.023915</td>\n",
              "      <td>0.014784</td>\n",
              "      <td>-0.024153</td>\n",
              "      <td>0.014272</td>\n",
              "      <td>-0.014024</td>\n",
              "      <td>0.014533</td>\n",
              "      <td>0.095729</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>-0.017059</td>\n",
              "      <td>0.109625</td>\n",
              "      <td>-0.000236</td>\n",
              "      <td>0.121168</td>\n",
              "      <td>0.154490</td>\n",
              "      <td>0.024470</td>\n",
              "      <td>-0.000973</td>\n",
              "      <td>0.041987</td>\n",
              "      <td>0.013063</td>\n",
              "      <td>0.041198</td>\n",
              "      <td>0.011177</td>\n",
              "      <td>0.023116</td>\n",
              "      <td>-0.001481</td>\n",
              "      <td>0.003744</td>\n",
              "      <td>0.099117</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>-0.018717</td>\n",
              "      <td>-0.008728</td>\n",
              "      <td>0.017194</td>\n",
              "      <td>0.000544</td>\n",
              "      <td>0.077427</td>\n",
              "      <td>-0.009547</td>\n",
              "      <td>0.027409</td>\n",
              "      <td>0.003903</td>\n",
              "      <td>0.058095</td>\n",
              "      <td>0.005639</td>\n",
              "      <td>0.056720</td>\n",
              "      <td>-0.004408</td>\n",
              "      <td>0.029645</td>\n",
              "      <td>-0.030096</td>\n",
              "      <td>-0.092341</td>\n",
              "      <td>-0.000364</td>\n",
              "      <td>0.014018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.064245</td>\n",
              "      <td>0.024718</td>\n",
              "      <td>0.075855</td>\n",
              "      <td>0.139330</td>\n",
              "      <td>0.005543</td>\n",
              "      <td>0.012666</td>\n",
              "      <td>0.021487</td>\n",
              "      <td>0.034923</td>\n",
              "      <td>0.022738</td>\n",
              "      <td>0.036211</td>\n",
              "      <td>0.008657</td>\n",
              "      <td>0.014996</td>\n",
              "      <td>-0.015077</td>\n",
              "      <td>0.019458</td>\n",
              "      <td>0.00153</td>\n",
              "      <td>-0.002817</td>\n",
              "      <td>0.054452</td>\n",
              "      <td>-0.018922</td>\n",
              "      <td>0.051060</td>\n",
              "      <td>0.030282</td>\n",
              "      <td>0.013954</td>\n",
              "      <td>-0.013428</td>\n",
              "      <td>0.017544</td>\n",
              "      <td>-0.023915</td>\n",
              "      <td>0.014784</td>\n",
              "      <td>-0.024153</td>\n",
              "      <td>0.014272</td>\n",
              "      <td>-0.014024</td>\n",
              "      <td>0.014533</td>\n",
              "      <td>0.087221</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>-0.017059</td>\n",
              "      <td>0.118697</td>\n",
              "      <td>0.005796</td>\n",
              "      <td>0.126915</td>\n",
              "      <td>0.169612</td>\n",
              "      <td>0.024470</td>\n",
              "      <td>-0.000973</td>\n",
              "      <td>0.041987</td>\n",
              "      <td>0.013063</td>\n",
              "      <td>0.041198</td>\n",
              "      <td>0.011177</td>\n",
              "      <td>0.023116</td>\n",
              "      <td>-0.001481</td>\n",
              "      <td>0.003744</td>\n",
              "      <td>0.106679</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>-0.018717</td>\n",
              "      <td>0.009794</td>\n",
              "      <td>0.043639</td>\n",
              "      <td>0.024794</td>\n",
              "      <td>0.109047</td>\n",
              "      <td>-0.009547</td>\n",
              "      <td>0.027409</td>\n",
              "      <td>0.003903</td>\n",
              "      <td>0.058095</td>\n",
              "      <td>0.005639</td>\n",
              "      <td>0.056720</td>\n",
              "      <td>-0.004408</td>\n",
              "      <td>0.029645</td>\n",
              "      <td>-0.030096</td>\n",
              "      <td>-0.067763</td>\n",
              "      <td>-0.000364</td>\n",
              "      <td>0.014018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70820</th>\n",
              "      <td>77671</td>\n",
              "      <td>0.047532</td>\n",
              "      <td>-0.110554</td>\n",
              "      <td>0.196474</td>\n",
              "      <td>0.261890</td>\n",
              "      <td>-0.012670</td>\n",
              "      <td>0.204857</td>\n",
              "      <td>-0.011992</td>\n",
              "      <td>0.082482</td>\n",
              "      <td>0.070932</td>\n",
              "      <td>0.189418</td>\n",
              "      <td>-0.012588</td>\n",
              "      <td>0.154542</td>\n",
              "      <td>-0.182753</td>\n",
              "      <td>0.028790</td>\n",
              "      <td>0.00153</td>\n",
              "      <td>-0.558346</td>\n",
              "      <td>-0.152954</td>\n",
              "      <td>0.033928</td>\n",
              "      <td>-0.021223</td>\n",
              "      <td>-0.044146</td>\n",
              "      <td>0.054283</td>\n",
              "      <td>-0.150759</td>\n",
              "      <td>-0.013154</td>\n",
              "      <td>-0.075916</td>\n",
              "      <td>-0.301802</td>\n",
              "      <td>-0.122252</td>\n",
              "      <td>-0.191145</td>\n",
              "      <td>-0.166947</td>\n",
              "      <td>0.003031</td>\n",
              "      <td>-0.099811</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>0.439347</td>\n",
              "      <td>-0.105422</td>\n",
              "      <td>-0.076626</td>\n",
              "      <td>0.175251</td>\n",
              "      <td>0.217744</td>\n",
              "      <td>0.041613</td>\n",
              "      <td>0.054098</td>\n",
              "      <td>-0.025146</td>\n",
              "      <td>0.006566</td>\n",
              "      <td>-0.230870</td>\n",
              "      <td>0.067166</td>\n",
              "      <td>-0.203734</td>\n",
              "      <td>-0.012405</td>\n",
              "      <td>-0.179722</td>\n",
              "      <td>-0.071021</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>-0.118999</td>\n",
              "      <td>0.200487</td>\n",
              "      <td>-0.144483</td>\n",
              "      <td>0.217698</td>\n",
              "      <td>0.306036</td>\n",
              "      <td>-0.066953</td>\n",
              "      <td>0.355617</td>\n",
              "      <td>0.001163</td>\n",
              "      <td>0.158399</td>\n",
              "      <td>0.372733</td>\n",
              "      <td>0.311670</td>\n",
              "      <td>0.178557</td>\n",
              "      <td>0.321489</td>\n",
              "      <td>-0.185784</td>\n",
              "      <td>0.128601</td>\n",
              "      <td>-0.000364</td>\n",
              "      <td>-0.997693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70821</th>\n",
              "      <td>77672</td>\n",
              "      <td>0.057649</td>\n",
              "      <td>-0.105592</td>\n",
              "      <td>0.220227</td>\n",
              "      <td>0.284594</td>\n",
              "      <td>-0.012182</td>\n",
              "      <td>0.206823</td>\n",
              "      <td>-0.013887</td>\n",
              "      <td>0.091615</td>\n",
              "      <td>0.048774</td>\n",
              "      <td>0.215278</td>\n",
              "      <td>-0.010999</td>\n",
              "      <td>0.185102</td>\n",
              "      <td>-0.174802</td>\n",
              "      <td>0.059567</td>\n",
              "      <td>0.00153</td>\n",
              "      <td>-0.568724</td>\n",
              "      <td>-0.163082</td>\n",
              "      <td>0.036482</td>\n",
              "      <td>-0.043304</td>\n",
              "      <td>-0.049219</td>\n",
              "      <td>0.040164</td>\n",
              "      <td>-0.161945</td>\n",
              "      <td>-0.042853</td>\n",
              "      <td>-0.096645</td>\n",
              "      <td>-0.377955</td>\n",
              "      <td>-0.127309</td>\n",
              "      <td>-0.205245</td>\n",
              "      <td>-0.177073</td>\n",
              "      <td>0.007069</td>\n",
              "      <td>-0.111383</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>0.475242</td>\n",
              "      <td>-0.105433</td>\n",
              "      <td>-0.069110</td>\n",
              "      <td>0.176924</td>\n",
              "      <td>0.235375</td>\n",
              "      <td>0.027982</td>\n",
              "      <td>0.044878</td>\n",
              "      <td>-0.056740</td>\n",
              "      <td>-0.005030</td>\n",
              "      <td>-0.329180</td>\n",
              "      <td>0.087970</td>\n",
              "      <td>-0.216244</td>\n",
              "      <td>0.008028</td>\n",
              "      <td>-0.167733</td>\n",
              "      <td>-0.051815</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>-0.093483</td>\n",
              "      <td>0.220732</td>\n",
              "      <td>-0.142074</td>\n",
              "      <td>0.263531</td>\n",
              "      <td>0.333812</td>\n",
              "      <td>-0.052346</td>\n",
              "      <td>0.368768</td>\n",
              "      <td>0.028965</td>\n",
              "      <td>0.188260</td>\n",
              "      <td>0.426729</td>\n",
              "      <td>0.342587</td>\n",
              "      <td>0.194246</td>\n",
              "      <td>0.362175</td>\n",
              "      <td>-0.181871</td>\n",
              "      <td>0.170950</td>\n",
              "      <td>-0.000364</td>\n",
              "      <td>-1.043966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70822</th>\n",
              "      <td>77673</td>\n",
              "      <td>0.066079</td>\n",
              "      <td>-0.099071</td>\n",
              "      <td>0.244751</td>\n",
              "      <td>0.306901</td>\n",
              "      <td>-0.008598</td>\n",
              "      <td>0.210741</td>\n",
              "      <td>-0.014197</td>\n",
              "      <td>0.102570</td>\n",
              "      <td>0.030423</td>\n",
              "      <td>0.249247</td>\n",
              "      <td>-0.002708</td>\n",
              "      <td>0.221406</td>\n",
              "      <td>-0.149129</td>\n",
              "      <td>0.109211</td>\n",
              "      <td>0.00153</td>\n",
              "      <td>-0.563941</td>\n",
              "      <td>-0.169730</td>\n",
              "      <td>0.042060</td>\n",
              "      <td>-0.065157</td>\n",
              "      <td>-0.052584</td>\n",
              "      <td>0.022522</td>\n",
              "      <td>-0.175324</td>\n",
              "      <td>-0.076534</td>\n",
              "      <td>-0.119258</td>\n",
              "      <td>-0.469808</td>\n",
              "      <td>-0.136817</td>\n",
              "      <td>-0.223019</td>\n",
              "      <td>-0.189165</td>\n",
              "      <td>-0.001855</td>\n",
              "      <td>-0.137473</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>0.502330</td>\n",
              "      <td>-0.103651</td>\n",
              "      <td>-0.057010</td>\n",
              "      <td>0.179594</td>\n",
              "      <td>0.254318</td>\n",
              "      <td>0.013924</td>\n",
              "      <td>0.035418</td>\n",
              "      <td>-0.090732</td>\n",
              "      <td>-0.016689</td>\n",
              "      <td>-0.439385</td>\n",
              "      <td>0.112429</td>\n",
              "      <td>-0.225728</td>\n",
              "      <td>0.032242</td>\n",
              "      <td>-0.150984</td>\n",
              "      <td>-0.028263</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>-0.061611</td>\n",
              "      <td>0.235810</td>\n",
              "      <td>-0.141131</td>\n",
              "      <td>0.309909</td>\n",
              "      <td>0.359485</td>\n",
              "      <td>-0.031121</td>\n",
              "      <td>0.386065</td>\n",
              "      <td>0.062337</td>\n",
              "      <td>0.221828</td>\n",
              "      <td>0.500230</td>\n",
              "      <td>0.386064</td>\n",
              "      <td>0.220311</td>\n",
              "      <td>0.410571</td>\n",
              "      <td>-0.147274</td>\n",
              "      <td>0.246684</td>\n",
              "      <td>-0.000364</td>\n",
              "      <td>-1.066270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70823</th>\n",
              "      <td>77674</td>\n",
              "      <td>0.074707</td>\n",
              "      <td>-0.088321</td>\n",
              "      <td>0.273382</td>\n",
              "      <td>0.332519</td>\n",
              "      <td>-0.001164</td>\n",
              "      <td>0.219474</td>\n",
              "      <td>-0.011959</td>\n",
              "      <td>0.116376</td>\n",
              "      <td>0.017980</td>\n",
              "      <td>0.293136</td>\n",
              "      <td>0.013737</td>\n",
              "      <td>0.264685</td>\n",
              "      <td>-0.108912</td>\n",
              "      <td>0.173457</td>\n",
              "      <td>0.00153</td>\n",
              "      <td>-0.546937</td>\n",
              "      <td>-0.174522</td>\n",
              "      <td>0.048707</td>\n",
              "      <td>-0.089601</td>\n",
              "      <td>-0.057659</td>\n",
              "      <td>0.001148</td>\n",
              "      <td>-0.193786</td>\n",
              "      <td>-0.114654</td>\n",
              "      <td>-0.144103</td>\n",
              "      <td>-0.580800</td>\n",
              "      <td>-0.154351</td>\n",
              "      <td>-0.247110</td>\n",
              "      <td>-0.205706</td>\n",
              "      <td>-0.024139</td>\n",
              "      <td>-0.177692</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>0.520364</td>\n",
              "      <td>-0.099815</td>\n",
              "      <td>-0.039613</td>\n",
              "      <td>0.183781</td>\n",
              "      <td>0.274860</td>\n",
              "      <td>-0.000015</td>\n",
              "      <td>0.025688</td>\n",
              "      <td>-0.126613</td>\n",
              "      <td>-0.027727</td>\n",
              "      <td>-0.562820</td>\n",
              "      <td>0.138785</td>\n",
              "      <td>-0.233372</td>\n",
              "      <td>0.058978</td>\n",
              "      <td>-0.133050</td>\n",
              "      <td>-0.004235</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>-0.026573</td>\n",
              "      <td>0.249229</td>\n",
              "      <td>-0.137028</td>\n",
              "      <td>0.362983</td>\n",
              "      <td>0.390177</td>\n",
              "      <td>-0.002312</td>\n",
              "      <td>0.413259</td>\n",
              "      <td>0.102695</td>\n",
              "      <td>0.260478</td>\n",
              "      <td>0.598779</td>\n",
              "      <td>0.447487</td>\n",
              "      <td>0.260847</td>\n",
              "      <td>0.470391</td>\n",
              "      <td>-0.084773</td>\n",
              "      <td>0.351150</td>\n",
              "      <td>-0.000364</td>\n",
              "      <td>-1.067302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70824</th>\n",
              "      <td>77675</td>\n",
              "      <td>0.085232</td>\n",
              "      <td>-0.071056</td>\n",
              "      <td>0.308663</td>\n",
              "      <td>0.364532</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.236596</td>\n",
              "      <td>-0.005506</td>\n",
              "      <td>0.134749</td>\n",
              "      <td>0.013395</td>\n",
              "      <td>0.348489</td>\n",
              "      <td>0.039668</td>\n",
              "      <td>0.316237</td>\n",
              "      <td>-0.058991</td>\n",
              "      <td>0.245986</td>\n",
              "      <td>0.00153</td>\n",
              "      <td>-0.521949</td>\n",
              "      <td>-0.178670</td>\n",
              "      <td>0.055071</td>\n",
              "      <td>-0.118442</td>\n",
              "      <td>-0.066747</td>\n",
              "      <td>-0.024737</td>\n",
              "      <td>-0.220770</td>\n",
              "      <td>-0.158433</td>\n",
              "      <td>-0.172156</td>\n",
              "      <td>-0.712860</td>\n",
              "      <td>-0.182920</td>\n",
              "      <td>-0.279660</td>\n",
              "      <td>-0.228881</td>\n",
              "      <td>-0.058738</td>\n",
              "      <td>-0.230053</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>0.530066</td>\n",
              "      <td>-0.093438</td>\n",
              "      <td>-0.015985</td>\n",
              "      <td>0.190222</td>\n",
              "      <td>0.297785</td>\n",
              "      <td>-0.013247</td>\n",
              "      <td>0.015826</td>\n",
              "      <td>-0.163939</td>\n",
              "      <td>-0.037407</td>\n",
              "      <td>-0.699466</td>\n",
              "      <td>0.165570</td>\n",
              "      <td>-0.239992</td>\n",
              "      <td>0.087356</td>\n",
              "      <td>-0.117729</td>\n",
              "      <td>0.015933</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>0.008117</td>\n",
              "      <td>0.263901</td>\n",
              "      <td>-0.126128</td>\n",
              "      <td>0.427105</td>\n",
              "      <td>0.431279</td>\n",
              "      <td>0.036227</td>\n",
              "      <td>0.457366</td>\n",
              "      <td>0.152927</td>\n",
              "      <td>0.306905</td>\n",
              "      <td>0.726255</td>\n",
              "      <td>0.531409</td>\n",
              "      <td>0.319328</td>\n",
              "      <td>0.545118</td>\n",
              "      <td>-0.000253</td>\n",
              "      <td>0.476040</td>\n",
              "      <td>-0.000364</td>\n",
              "      <td>-1.052015</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>70825 rows × 65 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       index   Hbo_Op1   Hbo_Op2  ...  Oxy_Op14  Oxy_Op15  Oxy_Op16\n",
              "0          0  0.024675 -0.015218  ... -0.049618 -0.000364  0.014018\n",
              "1          1  0.030511 -0.012349  ... -0.078378 -0.000364  0.014018\n",
              "2          2  0.039081 -0.004290  ... -0.094838 -0.000364  0.014018\n",
              "3          3  0.050448  0.008479  ... -0.092341 -0.000364  0.014018\n",
              "4          4  0.064245  0.024718  ... -0.067763 -0.000364  0.014018\n",
              "...      ...       ...       ...  ...       ...       ...       ...\n",
              "70820  77671  0.047532 -0.110554  ...  0.128601 -0.000364 -0.997693\n",
              "70821  77672  0.057649 -0.105592  ...  0.170950 -0.000364 -1.043966\n",
              "70822  77673  0.066079 -0.099071  ...  0.246684 -0.000364 -1.066270\n",
              "70823  77674  0.074707 -0.088321  ...  0.351150 -0.000364 -1.067302\n",
              "70824  77675  0.085232 -0.071056  ...  0.476040 -0.000364 -1.052015\n",
              "\n",
              "[70825 rows x 65 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 234
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj_QdnB8KdC-",
        "colab_type": "text"
      },
      "source": [
        "### Deneme 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfYttXMoKWyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns = data.columns\n",
        "def ImputeWithMean(df_data, cols=columns):\n",
        "\n",
        "  df_data = pd.DataFrame(df_data[cols])\n",
        "  mean_imputer = SimpleImputer(strategy=\"mean\")\n",
        "  mean_imputer.fit(df_data)\n",
        "    \n",
        "  imputed_df = mean_imputer.transform(df_data)\n",
        "  imputed_df = pd.DataFrame(imputed_df, columns=df_data.columns)\n",
        "\n",
        "  return imputed_df\n",
        "\n",
        "data_1 = ImputeWithMean(data_1)\n",
        "data_2 = ImputeWithMean(data_2)\n",
        "data_3 = ImputeWithMean(data_3)\n",
        "\n",
        "frames = [data_1, data_2, data_3]\n",
        "data = pd.concat(frames)\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HCcGhGIe3WH",
        "colab_type": "text"
      },
      "source": [
        "### Deneme 4\n",
        "\n",
        "16 seansların ortlamasını alarak da deneyebiliriz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_Jvexs3fTOO",
        "colab_type": "text"
      },
      "source": [
        "# Machine Learning Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gUYQiclfdja",
        "colab_type": "text"
      },
      "source": [
        "## Random Forests\n",
        "\n",
        "Isolation Forest ile yaptım."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0TBDLTRrSFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from keras.utils import np_utils\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "rnd = 50\n",
        "\n",
        "X = clean3.iloc[:,1:]\n",
        "\n",
        "indexis = clean3.iloc[:,0].to_list()\n",
        "yr = pd.DataFrame(data.iloc[:,67].copy())\n",
        "yr = pd.DataFrame(yr.loc[sel_index.to_list(),:].reset_index(drop = True))\n",
        "yr = pd.DataFrame(yr.iloc[indexis,:].reset_index(drop = True))\n",
        "\n",
        "yt = pd.DataFrame(data.iloc[:,68].copy())\n",
        "yt = pd.DataFrame(yt.loc[sel_index.to_list(),:].reset_index(drop = True))\n",
        "yt = pd.DataFrame(yt.iloc[indexis,:].reset_index(drop = True))\n",
        "\n",
        "\n",
        "#y_arr = np_utils.to_categorical(yr-1, num_classes=3)\n",
        "\n",
        "#for ResponseCode\n",
        "\n",
        "\n",
        "#y_arr = np_utils.to_categorical(yr-1, num_classes=3)\n",
        "X_arr = StandardScaler().fit_transform(X)\n",
        "\n",
        "X_trainr, X_testr, y_trainr, y_testr = train_test_split(X_arr, yr, test_size=0.3, random_state = rnd)\n",
        "\n",
        "#for TypeCode\n",
        "\n",
        "X_art = StandardScaler().fit_transform(X)\n",
        "\n",
        "X_traint, X_testt, y_traint, y_testt = train_test_split(X_art, yt, test_size=0.3, random_state = rnd)"
      ],
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi3Jqdp_qHqb",
        "colab_type": "text"
      },
      "source": [
        "### ResponesCode Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5fvHZsn9EMT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "8f2617fc-6c7a-4e72-f2fa-306e5317c595"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.model_selection import cross_val_score, cross_validate\n",
        "from time import time\n",
        "\n",
        "s1 = time()\n",
        "selector = SelectFromModel(RandomForestClassifier()).fit(X_trainr, y_trainr)\n",
        "s2 = time()\n",
        "print(\"Passed time for Feature Selection:\",s2-s1)\n",
        "X_rnd_trainr = selector.transform(X_trainr)\n",
        "X_rnd_testr = selector.transform(X_testr)\n",
        "\n",
        "print(\"Selected Columns:\",X.loc[:,selector.get_support()].columns.tolist(),\"\\n\")\n",
        "s3 = time()\n",
        "models = RandomForestClassifier(random_state = rnd)\n",
        "cv = cross_validate(models,X_rnd_trainr,y_trainr,cv = 3, n_jobs=-1, return_estimator=True)\n",
        "s4 = time()\n",
        "print(\"Mean training accuracy: {}\".format(np.mean(cv['test_score'])))\n",
        "print(\"Test accuracy: {}\".format(cv[\"estimator\"][0].score(X_rnd_testr,y_testr)))\n",
        "print(\"Passed time for Cross Validation:\",s4-s3)"
      ],
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selected Columns: ['Hbo_Op2', 'Hbo_Op16', 'Hbr_Op1', 'Hbr_Op2', 'Hbr_Op3', 'Hbr_Op4', 'Hbr_Op5', 'Hbr_Op6', 'Hbr_Op7', 'Hbr_Op8', 'Hbr_Op9', 'Hbr_Op12', 'Hbr_Op14', 'Hbr_Op16', 'Hbt_Op1', 'Hbt_Op2', 'Hbt_Op3', 'Hbt_Op4', 'Hbt_Op5', 'Hbt_Op6', 'Hbt_Op7', 'Hbt_Op8', 'Hbt_Op9', 'Hbt_Op10', 'Hbt_Op11', 'Hbt_Op12', 'Hbt_Op13', 'Hbt_Op14', 'Hbt_Op16', 'Oxy_Op1', 'Oxy_Op2', 'Oxy_Op4'] \n",
            "\n",
            "Mean training accuracy: 0.8804082883134258\n",
            "Test accuracy: 0.8828125\n",
            "Passed time for Cross Validation: 69.72065687179565\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "favDMg0wEc4L",
        "colab_type": "text"
      },
      "source": [
        "#### Hyper Parameter Tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfBNGjhNFBvT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "67182b96-313c-4cc6-9d46-0c8abade682b"
      },
      "source": [
        "#uzun sürer diye çalıştırmadım\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
        "\n",
        "param_rnd = {\"min_samples_split\" : [2,10,50],\n",
        "             \"criterion\" : [\"gini\",\"entropy\"],\n",
        "             \"ccp_alpha\": [1e-5,1e-4,1e-3,0.01,0.1,0] }\n",
        "\n",
        "\n",
        "ran_forest = RandomForestClassifier(random_state = rnd)\n",
        "\n",
        "clf_rnd = GridSearchCV(ran_forest, param_rnd, cv=3, n_jobs = -1)\n",
        "clf_rnd.fit(X_rnd_trainr, y_trainr)\n",
        "\n",
        "print(\"Tuned Random Forest Classification Parameters: {}\".format(clf_rnd.best_params_)) \n",
        "print(\"Mean of the cv scores is {:.6f}\".format(clf_rnd.best_score_))\n",
        "print(\"Test Score {:.6f}\".format(clf_rnd.score(X_rnd_testr,y_testr)))\n",
        "print(\"Seconds used for refitting the best model on the train dataset: {:.6f}\".format(clf_rnd.refit_time_))"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-202-7dac1bfbb62c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mclf_rnd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mran_forest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_rnd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mclf_rnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_rnd_trainr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trainr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tuned Random Forest Classification Parameters: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf_rnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    538\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAOsBw4FFhd2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "fdf4d1a5-1c32-4770-cc47-4df618da3dd8"
      },
      "source": [
        "mmodel = RandomForestClassifier(ccp_alpha=0.00001,random_state = rnd)\n",
        "cv = cross_validate(mmodel,X_rnd_trainr,y_trainr,cv = 3, n_jobs=-1, return_estimator=True)\n",
        "print(\"Mean training accuracy: {}\".format(np.mean(cv['test_score'])))\n",
        "print(\"Test accuracy: {}\".format(cv[\"estimator\"][0].score(X_rnd_testr,y_testr)))"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean training accuracy: 0.8776069081937875\n",
            "Test accuracy: 0.8800629740947474\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgzokozpIl8c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "335f8528-43f0-43db-a542-e50532b0309a"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_pred_rndr = cv[\"estimator\"][0].predict(X_rnd_testr)\n",
        "print(classification_report(y_testr,y_pred_rndr))"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.88      0.90      0.89      8579\n",
            "         2.0       0.98      0.61      0.75      2546\n",
            "         3.0       0.87      0.93      0.90      9836\n",
            "\n",
            "    accuracy                           0.88     20961\n",
            "   macro avg       0.91      0.81      0.85     20961\n",
            "weighted avg       0.89      0.88      0.88     20961\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU8fqdxqJoM-",
        "colab_type": "text"
      },
      "source": [
        "### TypeCode Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBixiI3zJV6G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "3cb66697-4740-49bf-a484-e5e9eda48945"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.model_selection import cross_val_score, cross_validate\n",
        "from time import time\n",
        "\n",
        "s1 = time()\n",
        "selector2 = SelectFromModel(RandomForestClassifier()).fit(X_traint, y_traint)\n",
        "s2 = time()\n",
        "print(\"Passed time for Feature Selection:\",s2-s1)\n",
        "X_rnd_traint = selector2.transform(X_traint)\n",
        "X_rnd_testt = selector2.transform(X_testt)\n",
        "\n",
        "print(\"Selected Columns:\",X.loc[:,selector2.get_support()].columns.tolist(),\"\\n\")\n",
        "s3 = time()\n",
        "models = RandomForestClassifier(random_state = rnd)\n",
        "cv = cross_validate(models,X_rnd_traint,y_traint,cv = 3, n_jobs=-1, return_estimator=True)\n",
        "s4 = time()\n",
        "print(\"Mean training accuracy: {}\".format(np.mean(cv['test_score'])))\n",
        "print(\"Test accuracy: {}\".format(cv[\"estimator\"][0].score(X_rnd_testt,y_testt)))\n",
        "print(\"Passed time for Cross Validation:\",s4-s3)"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Passed time for Feature Selection: 62.641599893569946\n",
            "Selected Columns: ['Hbo_Op2', 'Hbo_Op16', 'Hbr_Op1', 'Hbr_Op2', 'Hbr_Op3', 'Hbr_Op4', 'Hbr_Op5', 'Hbr_Op6', 'Hbr_Op7', 'Hbr_Op8', 'Hbr_Op9', 'Hbr_Op10', 'Hbr_Op11', 'Hbr_Op12', 'Hbr_Op14', 'Hbr_Op16', 'Hbt_Op1', 'Hbt_Op2', 'Hbt_Op3', 'Hbt_Op4', 'Hbt_Op5', 'Hbt_Op6', 'Hbt_Op7', 'Hbt_Op8', 'Hbt_Op9', 'Hbt_Op10', 'Hbt_Op11', 'Hbt_Op12', 'Hbt_Op13', 'Hbt_Op14', 'Hbt_Op16', 'Oxy_Op2', 'Oxy_Op3', 'Oxy_Op4'] \n",
            "\n",
            "Mean training accuracy: 0.8967448589717013\n",
            "Test accuracy: 0.8948046371833405\n",
            "Passed time for Cross Validation: 76.36390590667725\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXQDAk5jNuHT",
        "colab_type": "text"
      },
      "source": [
        "#### Hyperparamter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBJ0DvAXNpXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#uzun sürer diye çalıştırmadım\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
        "\n",
        "param_rnd = {\"min_samples_split\" : [2,10,50],\n",
        "             \"criterion\" : [\"gini\",\"entropy\"],\n",
        "             \"ccp_alpha\": [1e-5,1e-4,1e-3,0.01,0.1,0] }\n",
        "\n",
        "\n",
        "ran_forest = RandomForestClassifier(random_state = rnd)\n",
        "\n",
        "clf_rnd2 = GridSearchCV(ran_forest, param_rnd, cv=3, n_jobs = -1)\n",
        "clf_rnd2.fit(X_rnd_traint, y_traint)\n",
        "\n",
        "print(\"Tuned Random Forest Classification Parameters: {}\".format(clf_rnd2.best_params_)) \n",
        "print(\"Mean of the cv scores is {:.6f}\".format(clf_rnd2.best_score_))\n",
        "print(\"Test Score {:.6f}\".format(clf_rnd2.score(X_rnd_testt,y_testt)))\n",
        "print(\"Seconds used for refitting the best model on the train dataset: {:.6f}\".format(clf_rnd2.refit_time_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx4UdS17IMsU",
        "colab_type": "text"
      },
      "source": [
        "## Random Forests(z-score outlier deleting)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkvFNslQMFB-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "109115d2-dbbe-430e-81da-c7af98ff4e52"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60088, 63)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 229
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOsPEI8oe1Hm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "cb4d45d7-12e8-4121-829c-2e7bb8a2afc1"
      },
      "source": [
        "rnd = 50\n",
        "\n",
        "X = clean2.iloc[:,1:]\n",
        "\n",
        "yr = pd.DataFrame(data.iloc[:,67].copy())\n",
        "yr = pd.DataFrame(yr.loc[sel_index.to_list(),:].reset_index(drop = True))\n",
        "yr = yr.loc[sel_index2.to_list(),:].reset_index(drop = True)\n",
        "yr = yr.to_clipboard\n",
        "\n",
        "yt = pd.DataFrame(data.iloc[:,68].copy())\n",
        "yt = pd.DataFrame(yt.loc[sel_index.to_list(),:].reset_index(drop = True))\n",
        "yt = yt.loc[sel_index2.to_list(),:].reset_index(drop = True)\n",
        "\n",
        "\n",
        "#for ResponseCode\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from keras.utils import np_utils\n",
        "\n",
        "y_arr = np_utils.to_categorical(yr-1, num_classes=3)\n",
        "X_arr = StandardScaler().fit_transform(X)\n",
        "\n",
        "X_trainr, X_testr, y_trainr, y_testr = train_test_split(X_arr, y_arr, test_size=0.3, random_state = 127)\n",
        "\n",
        "#for TypeCode\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from keras.utils import np_utils\n",
        "\n",
        "y_art = np_utils.to_categorical(yt-1, num_classes=3)\n",
        "X_art = StandardScaler().fit_transform(X)\n",
        "\n",
        "X_traint, X_testt, y_traint, y_testt = train_test_split(X_art, y_art, test_size=0.3, random_state = rnd)"
      ],
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Hbo_Op1</th>\n",
              "      <th>Hbo_Op2</th>\n",
              "      <th>Hbo_Op3</th>\n",
              "      <th>Hbo_Op4</th>\n",
              "      <th>Hbo_Op5</th>\n",
              "      <th>Hbo_Op6</th>\n",
              "      <th>Hbo_Op7</th>\n",
              "      <th>Hbo_Op8</th>\n",
              "      <th>Hbo_Op9</th>\n",
              "      <th>Hbo_Op10</th>\n",
              "      <th>Hbo_Op11</th>\n",
              "      <th>Hbo_Op12</th>\n",
              "      <th>Hbo_Op13</th>\n",
              "      <th>Hbo_Op14</th>\n",
              "      <th>Hbo_Op15</th>\n",
              "      <th>Hbo_Op16</th>\n",
              "      <th>Hbr_Op1</th>\n",
              "      <th>Hbr_Op2</th>\n",
              "      <th>Hbr_Op3</th>\n",
              "      <th>Hbr_Op4</th>\n",
              "      <th>Hbr_Op5</th>\n",
              "      <th>Hbr_Op6</th>\n",
              "      <th>Hbr_Op7</th>\n",
              "      <th>Hbr_Op8</th>\n",
              "      <th>Hbr_Op9</th>\n",
              "      <th>Hbr_Op10</th>\n",
              "      <th>Hbr_Op11</th>\n",
              "      <th>Hbr_Op12</th>\n",
              "      <th>Hbr_Op13</th>\n",
              "      <th>Hbr_Op14</th>\n",
              "      <th>Hbr_Op15</th>\n",
              "      <th>Hbr_Op16</th>\n",
              "      <th>Hbt_Op1</th>\n",
              "      <th>Hbt_Op2</th>\n",
              "      <th>Hbt_Op3</th>\n",
              "      <th>Hbt_Op4</th>\n",
              "      <th>Hbt_Op5</th>\n",
              "      <th>Hbt_Op6</th>\n",
              "      <th>Hbt_Op7</th>\n",
              "      <th>Hbt_Op8</th>\n",
              "      <th>Hbt_Op9</th>\n",
              "      <th>Hbt_Op10</th>\n",
              "      <th>Hbt_Op11</th>\n",
              "      <th>Hbt_Op12</th>\n",
              "      <th>Hbt_Op13</th>\n",
              "      <th>Hbt_Op14</th>\n",
              "      <th>Hbt_Op15</th>\n",
              "      <th>Hbt_Op16</th>\n",
              "      <th>Oxy_Op1</th>\n",
              "      <th>Oxy_Op2</th>\n",
              "      <th>Oxy_Op3</th>\n",
              "      <th>Oxy_Op4</th>\n",
              "      <th>Oxy_Op5</th>\n",
              "      <th>Oxy_Op6</th>\n",
              "      <th>Oxy_Op7</th>\n",
              "      <th>Oxy_Op8</th>\n",
              "      <th>Oxy_Op9</th>\n",
              "      <th>Oxy_Op10</th>\n",
              "      <th>Oxy_Op11</th>\n",
              "      <th>Oxy_Op12</th>\n",
              "      <th>Oxy_Op13</th>\n",
              "      <th>Oxy_Op14</th>\n",
              "      <th>Oxy_Op15</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.024675</td>\n",
              "      <td>-0.015218</td>\n",
              "      <td>0.032464</td>\n",
              "      <td>0.058835</td>\n",
              "      <td>0.005543</td>\n",
              "      <td>0.012666</td>\n",
              "      <td>0.021487</td>\n",
              "      <td>0.034923</td>\n",
              "      <td>0.022738</td>\n",
              "      <td>0.036211</td>\n",
              "      <td>0.008657</td>\n",
              "      <td>0.014996</td>\n",
              "      <td>-0.015077</td>\n",
              "      <td>0.002910</td>\n",
              "      <td>0.00153</td>\n",
              "      <td>-0.002817</td>\n",
              "      <td>0.045010</td>\n",
              "      <td>0.001022</td>\n",
              "      <td>0.052551</td>\n",
              "      <td>0.035209</td>\n",
              "      <td>0.013954</td>\n",
              "      <td>-0.013428</td>\n",
              "      <td>0.017544</td>\n",
              "      <td>-0.023915</td>\n",
              "      <td>0.014784</td>\n",
              "      <td>-0.024153</td>\n",
              "      <td>0.014272</td>\n",
              "      <td>-0.014024</td>\n",
              "      <td>0.014533</td>\n",
              "      <td>0.052528</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>-0.017059</td>\n",
              "      <td>0.069685</td>\n",
              "      <td>-0.014196</td>\n",
              "      <td>0.085015</td>\n",
              "      <td>0.094044</td>\n",
              "      <td>0.024470</td>\n",
              "      <td>-0.000973</td>\n",
              "      <td>0.041987</td>\n",
              "      <td>0.013063</td>\n",
              "      <td>0.041198</td>\n",
              "      <td>0.011177</td>\n",
              "      <td>0.023116</td>\n",
              "      <td>-0.001481</td>\n",
              "      <td>0.003744</td>\n",
              "      <td>0.055438</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>-0.018717</td>\n",
              "      <td>-0.020335</td>\n",
              "      <td>-0.016239</td>\n",
              "      <td>-0.020087</td>\n",
              "      <td>0.023625</td>\n",
              "      <td>-0.009547</td>\n",
              "      <td>0.027409</td>\n",
              "      <td>0.003903</td>\n",
              "      <td>0.058095</td>\n",
              "      <td>0.005639</td>\n",
              "      <td>0.056720</td>\n",
              "      <td>-0.004408</td>\n",
              "      <td>0.029645</td>\n",
              "      <td>-0.030096</td>\n",
              "      <td>-0.049618</td>\n",
              "      <td>-0.000364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.030511</td>\n",
              "      <td>-0.012349</td>\n",
              "      <td>0.039233</td>\n",
              "      <td>0.074926</td>\n",
              "      <td>0.005543</td>\n",
              "      <td>0.012666</td>\n",
              "      <td>0.021487</td>\n",
              "      <td>0.034923</td>\n",
              "      <td>0.022738</td>\n",
              "      <td>0.036211</td>\n",
              "      <td>0.008657</td>\n",
              "      <td>0.014996</td>\n",
              "      <td>-0.015077</td>\n",
              "      <td>-0.003348</td>\n",
              "      <td>0.00153</td>\n",
              "      <td>-0.002817</td>\n",
              "      <td>0.053890</td>\n",
              "      <td>0.000589</td>\n",
              "      <td>0.060653</td>\n",
              "      <td>0.040470</td>\n",
              "      <td>0.013954</td>\n",
              "      <td>-0.013428</td>\n",
              "      <td>0.017544</td>\n",
              "      <td>-0.023915</td>\n",
              "      <td>0.014784</td>\n",
              "      <td>-0.024153</td>\n",
              "      <td>0.014272</td>\n",
              "      <td>-0.014024</td>\n",
              "      <td>0.014533</td>\n",
              "      <td>0.075030</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>-0.017059</td>\n",
              "      <td>0.084401</td>\n",
              "      <td>-0.011760</td>\n",
              "      <td>0.099886</td>\n",
              "      <td>0.115395</td>\n",
              "      <td>0.024470</td>\n",
              "      <td>-0.000973</td>\n",
              "      <td>0.041987</td>\n",
              "      <td>0.013063</td>\n",
              "      <td>0.041198</td>\n",
              "      <td>0.011177</td>\n",
              "      <td>0.023116</td>\n",
              "      <td>-0.001481</td>\n",
              "      <td>0.003744</td>\n",
              "      <td>0.071682</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>-0.018717</td>\n",
              "      <td>-0.023379</td>\n",
              "      <td>-0.012938</td>\n",
              "      <td>-0.021419</td>\n",
              "      <td>0.034456</td>\n",
              "      <td>-0.009547</td>\n",
              "      <td>0.027409</td>\n",
              "      <td>0.003903</td>\n",
              "      <td>0.058095</td>\n",
              "      <td>0.005639</td>\n",
              "      <td>0.056720</td>\n",
              "      <td>-0.004408</td>\n",
              "      <td>0.029645</td>\n",
              "      <td>-0.030096</td>\n",
              "      <td>-0.078378</td>\n",
              "      <td>-0.000364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.039081</td>\n",
              "      <td>-0.004290</td>\n",
              "      <td>0.048589</td>\n",
              "      <td>0.094117</td>\n",
              "      <td>0.005543</td>\n",
              "      <td>0.012666</td>\n",
              "      <td>0.021487</td>\n",
              "      <td>0.034923</td>\n",
              "      <td>0.022738</td>\n",
              "      <td>0.036211</td>\n",
              "      <td>0.008657</td>\n",
              "      <td>0.014996</td>\n",
              "      <td>-0.015077</td>\n",
              "      <td>-0.003982</td>\n",
              "      <td>0.00153</td>\n",
              "      <td>-0.002817</td>\n",
              "      <td>0.058895</td>\n",
              "      <td>-0.002358</td>\n",
              "      <td>0.063509</td>\n",
              "      <td>0.041851</td>\n",
              "      <td>0.013954</td>\n",
              "      <td>-0.013428</td>\n",
              "      <td>0.017544</td>\n",
              "      <td>-0.023915</td>\n",
              "      <td>0.014784</td>\n",
              "      <td>-0.024153</td>\n",
              "      <td>0.014272</td>\n",
              "      <td>-0.014024</td>\n",
              "      <td>0.014533</td>\n",
              "      <td>0.090856</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>-0.017059</td>\n",
              "      <td>0.097976</td>\n",
              "      <td>-0.006647</td>\n",
              "      <td>0.112098</td>\n",
              "      <td>0.135968</td>\n",
              "      <td>0.024470</td>\n",
              "      <td>-0.000973</td>\n",
              "      <td>0.041987</td>\n",
              "      <td>0.013063</td>\n",
              "      <td>0.041198</td>\n",
              "      <td>0.011177</td>\n",
              "      <td>0.023116</td>\n",
              "      <td>-0.001481</td>\n",
              "      <td>0.003744</td>\n",
              "      <td>0.086874</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>-0.018717</td>\n",
              "      <td>-0.019813</td>\n",
              "      <td>-0.001932</td>\n",
              "      <td>-0.014920</td>\n",
              "      <td>0.052266</td>\n",
              "      <td>-0.009547</td>\n",
              "      <td>0.027409</td>\n",
              "      <td>0.003903</td>\n",
              "      <td>0.058095</td>\n",
              "      <td>0.005639</td>\n",
              "      <td>0.056720</td>\n",
              "      <td>-0.004408</td>\n",
              "      <td>0.029645</td>\n",
              "      <td>-0.030096</td>\n",
              "      <td>-0.094838</td>\n",
              "      <td>-0.000364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.050448</td>\n",
              "      <td>0.008479</td>\n",
              "      <td>0.060856</td>\n",
              "      <td>0.115959</td>\n",
              "      <td>0.005543</td>\n",
              "      <td>0.012666</td>\n",
              "      <td>0.021487</td>\n",
              "      <td>0.034923</td>\n",
              "      <td>0.022738</td>\n",
              "      <td>0.036211</td>\n",
              "      <td>0.008657</td>\n",
              "      <td>0.014996</td>\n",
              "      <td>-0.015077</td>\n",
              "      <td>0.003388</td>\n",
              "      <td>0.00153</td>\n",
              "      <td>-0.002817</td>\n",
              "      <td>0.059176</td>\n",
              "      <td>-0.008715</td>\n",
              "      <td>0.060312</td>\n",
              "      <td>0.038532</td>\n",
              "      <td>0.013954</td>\n",
              "      <td>-0.013428</td>\n",
              "      <td>0.017544</td>\n",
              "      <td>-0.023915</td>\n",
              "      <td>0.014784</td>\n",
              "      <td>-0.024153</td>\n",
              "      <td>0.014272</td>\n",
              "      <td>-0.014024</td>\n",
              "      <td>0.014533</td>\n",
              "      <td>0.095729</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>-0.017059</td>\n",
              "      <td>0.109625</td>\n",
              "      <td>-0.000236</td>\n",
              "      <td>0.121168</td>\n",
              "      <td>0.154490</td>\n",
              "      <td>0.024470</td>\n",
              "      <td>-0.000973</td>\n",
              "      <td>0.041987</td>\n",
              "      <td>0.013063</td>\n",
              "      <td>0.041198</td>\n",
              "      <td>0.011177</td>\n",
              "      <td>0.023116</td>\n",
              "      <td>-0.001481</td>\n",
              "      <td>0.003744</td>\n",
              "      <td>0.099117</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>-0.018717</td>\n",
              "      <td>-0.008728</td>\n",
              "      <td>0.017194</td>\n",
              "      <td>0.000544</td>\n",
              "      <td>0.077427</td>\n",
              "      <td>-0.009547</td>\n",
              "      <td>0.027409</td>\n",
              "      <td>0.003903</td>\n",
              "      <td>0.058095</td>\n",
              "      <td>0.005639</td>\n",
              "      <td>0.056720</td>\n",
              "      <td>-0.004408</td>\n",
              "      <td>0.029645</td>\n",
              "      <td>-0.030096</td>\n",
              "      <td>-0.092341</td>\n",
              "      <td>-0.000364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.064245</td>\n",
              "      <td>0.024718</td>\n",
              "      <td>0.075855</td>\n",
              "      <td>0.139330</td>\n",
              "      <td>0.005543</td>\n",
              "      <td>0.012666</td>\n",
              "      <td>0.021487</td>\n",
              "      <td>0.034923</td>\n",
              "      <td>0.022738</td>\n",
              "      <td>0.036211</td>\n",
              "      <td>0.008657</td>\n",
              "      <td>0.014996</td>\n",
              "      <td>-0.015077</td>\n",
              "      <td>0.019458</td>\n",
              "      <td>0.00153</td>\n",
              "      <td>-0.002817</td>\n",
              "      <td>0.054452</td>\n",
              "      <td>-0.018922</td>\n",
              "      <td>0.051060</td>\n",
              "      <td>0.030282</td>\n",
              "      <td>0.013954</td>\n",
              "      <td>-0.013428</td>\n",
              "      <td>0.017544</td>\n",
              "      <td>-0.023915</td>\n",
              "      <td>0.014784</td>\n",
              "      <td>-0.024153</td>\n",
              "      <td>0.014272</td>\n",
              "      <td>-0.014024</td>\n",
              "      <td>0.014533</td>\n",
              "      <td>0.087221</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>-0.017059</td>\n",
              "      <td>0.118697</td>\n",
              "      <td>0.005796</td>\n",
              "      <td>0.126915</td>\n",
              "      <td>0.169612</td>\n",
              "      <td>0.024470</td>\n",
              "      <td>-0.000973</td>\n",
              "      <td>0.041987</td>\n",
              "      <td>0.013063</td>\n",
              "      <td>0.041198</td>\n",
              "      <td>0.011177</td>\n",
              "      <td>0.023116</td>\n",
              "      <td>-0.001481</td>\n",
              "      <td>0.003744</td>\n",
              "      <td>0.106679</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>-0.018717</td>\n",
              "      <td>0.009794</td>\n",
              "      <td>0.043639</td>\n",
              "      <td>0.024794</td>\n",
              "      <td>0.109047</td>\n",
              "      <td>-0.009547</td>\n",
              "      <td>0.027409</td>\n",
              "      <td>0.003903</td>\n",
              "      <td>0.058095</td>\n",
              "      <td>0.005639</td>\n",
              "      <td>0.056720</td>\n",
              "      <td>-0.004408</td>\n",
              "      <td>0.029645</td>\n",
              "      <td>-0.030096</td>\n",
              "      <td>-0.067763</td>\n",
              "      <td>-0.000364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60083</th>\n",
              "      <td>77670</td>\n",
              "      <td>0.034821</td>\n",
              "      <td>-0.115473</td>\n",
              "      <td>0.171372</td>\n",
              "      <td>0.236536</td>\n",
              "      <td>-0.010133</td>\n",
              "      <td>0.203421</td>\n",
              "      <td>-0.008541</td>\n",
              "      <td>0.075152</td>\n",
              "      <td>0.095367</td>\n",
              "      <td>0.170726</td>\n",
              "      <td>-0.008445</td>\n",
              "      <td>0.129385</td>\n",
              "      <td>-0.171005</td>\n",
              "      <td>0.020026</td>\n",
              "      <td>0.00153</td>\n",
              "      <td>-0.530636</td>\n",
              "      <td>-0.138584</td>\n",
              "      <td>0.035375</td>\n",
              "      <td>0.002690</td>\n",
              "      <td>-0.035139</td>\n",
              "      <td>0.064553</td>\n",
              "      <td>-0.140066</td>\n",
              "      <td>0.012067</td>\n",
              "      <td>-0.057594</td>\n",
              "      <td>-0.238289</td>\n",
              "      <td>-0.119395</td>\n",
              "      <td>-0.178760</td>\n",
              "      <td>-0.157317</td>\n",
              "      <td>-0.012186</td>\n",
              "      <td>-0.101482</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>0.395693</td>\n",
              "      <td>-0.103764</td>\n",
              "      <td>-0.080098</td>\n",
              "      <td>0.174061</td>\n",
              "      <td>0.201397</td>\n",
              "      <td>0.054420</td>\n",
              "      <td>0.063355</td>\n",
              "      <td>0.003526</td>\n",
              "      <td>0.017558</td>\n",
              "      <td>-0.142922</td>\n",
              "      <td>0.051331</td>\n",
              "      <td>-0.187205</td>\n",
              "      <td>-0.027932</td>\n",
              "      <td>-0.183190</td>\n",
              "      <td>-0.081456</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>-0.134942</td>\n",
              "      <td>0.173405</td>\n",
              "      <td>-0.150848</td>\n",
              "      <td>0.168682</td>\n",
              "      <td>0.271675</td>\n",
              "      <td>-0.074685</td>\n",
              "      <td>0.343487</td>\n",
              "      <td>-0.020608</td>\n",
              "      <td>0.132747</td>\n",
              "      <td>0.333656</td>\n",
              "      <td>0.290121</td>\n",
              "      <td>0.170315</td>\n",
              "      <td>0.286703</td>\n",
              "      <td>-0.158819</td>\n",
              "      <td>0.121507</td>\n",
              "      <td>-0.000364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60084</th>\n",
              "      <td>77671</td>\n",
              "      <td>0.047532</td>\n",
              "      <td>-0.110554</td>\n",
              "      <td>0.196474</td>\n",
              "      <td>0.261890</td>\n",
              "      <td>-0.012670</td>\n",
              "      <td>0.204857</td>\n",
              "      <td>-0.011992</td>\n",
              "      <td>0.082482</td>\n",
              "      <td>0.070932</td>\n",
              "      <td>0.189418</td>\n",
              "      <td>-0.012588</td>\n",
              "      <td>0.154542</td>\n",
              "      <td>-0.182753</td>\n",
              "      <td>0.028790</td>\n",
              "      <td>0.00153</td>\n",
              "      <td>-0.558346</td>\n",
              "      <td>-0.152954</td>\n",
              "      <td>0.033928</td>\n",
              "      <td>-0.021223</td>\n",
              "      <td>-0.044146</td>\n",
              "      <td>0.054283</td>\n",
              "      <td>-0.150759</td>\n",
              "      <td>-0.013154</td>\n",
              "      <td>-0.075916</td>\n",
              "      <td>-0.301802</td>\n",
              "      <td>-0.122252</td>\n",
              "      <td>-0.191145</td>\n",
              "      <td>-0.166947</td>\n",
              "      <td>0.003031</td>\n",
              "      <td>-0.099811</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>0.439347</td>\n",
              "      <td>-0.105422</td>\n",
              "      <td>-0.076626</td>\n",
              "      <td>0.175251</td>\n",
              "      <td>0.217744</td>\n",
              "      <td>0.041613</td>\n",
              "      <td>0.054098</td>\n",
              "      <td>-0.025146</td>\n",
              "      <td>0.006566</td>\n",
              "      <td>-0.230870</td>\n",
              "      <td>0.067166</td>\n",
              "      <td>-0.203734</td>\n",
              "      <td>-0.012405</td>\n",
              "      <td>-0.179722</td>\n",
              "      <td>-0.071021</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>-0.118999</td>\n",
              "      <td>0.200487</td>\n",
              "      <td>-0.144483</td>\n",
              "      <td>0.217698</td>\n",
              "      <td>0.306036</td>\n",
              "      <td>-0.066953</td>\n",
              "      <td>0.355617</td>\n",
              "      <td>0.001163</td>\n",
              "      <td>0.158399</td>\n",
              "      <td>0.372733</td>\n",
              "      <td>0.311670</td>\n",
              "      <td>0.178557</td>\n",
              "      <td>0.321489</td>\n",
              "      <td>-0.185784</td>\n",
              "      <td>0.128601</td>\n",
              "      <td>-0.000364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60085</th>\n",
              "      <td>77672</td>\n",
              "      <td>0.057649</td>\n",
              "      <td>-0.105592</td>\n",
              "      <td>0.220227</td>\n",
              "      <td>0.284594</td>\n",
              "      <td>-0.012182</td>\n",
              "      <td>0.206823</td>\n",
              "      <td>-0.013887</td>\n",
              "      <td>0.091615</td>\n",
              "      <td>0.048774</td>\n",
              "      <td>0.215278</td>\n",
              "      <td>-0.010999</td>\n",
              "      <td>0.185102</td>\n",
              "      <td>-0.174802</td>\n",
              "      <td>0.059567</td>\n",
              "      <td>0.00153</td>\n",
              "      <td>-0.568724</td>\n",
              "      <td>-0.163082</td>\n",
              "      <td>0.036482</td>\n",
              "      <td>-0.043304</td>\n",
              "      <td>-0.049219</td>\n",
              "      <td>0.040164</td>\n",
              "      <td>-0.161945</td>\n",
              "      <td>-0.042853</td>\n",
              "      <td>-0.096645</td>\n",
              "      <td>-0.377955</td>\n",
              "      <td>-0.127309</td>\n",
              "      <td>-0.205245</td>\n",
              "      <td>-0.177073</td>\n",
              "      <td>0.007069</td>\n",
              "      <td>-0.111383</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>0.475242</td>\n",
              "      <td>-0.105433</td>\n",
              "      <td>-0.069110</td>\n",
              "      <td>0.176924</td>\n",
              "      <td>0.235375</td>\n",
              "      <td>0.027982</td>\n",
              "      <td>0.044878</td>\n",
              "      <td>-0.056740</td>\n",
              "      <td>-0.005030</td>\n",
              "      <td>-0.329180</td>\n",
              "      <td>0.087970</td>\n",
              "      <td>-0.216244</td>\n",
              "      <td>0.008028</td>\n",
              "      <td>-0.167733</td>\n",
              "      <td>-0.051815</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>-0.093483</td>\n",
              "      <td>0.220732</td>\n",
              "      <td>-0.142074</td>\n",
              "      <td>0.263531</td>\n",
              "      <td>0.333812</td>\n",
              "      <td>-0.052346</td>\n",
              "      <td>0.368768</td>\n",
              "      <td>0.028965</td>\n",
              "      <td>0.188260</td>\n",
              "      <td>0.426729</td>\n",
              "      <td>0.342587</td>\n",
              "      <td>0.194246</td>\n",
              "      <td>0.362175</td>\n",
              "      <td>-0.181871</td>\n",
              "      <td>0.170950</td>\n",
              "      <td>-0.000364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60086</th>\n",
              "      <td>77673</td>\n",
              "      <td>0.066079</td>\n",
              "      <td>-0.099071</td>\n",
              "      <td>0.244751</td>\n",
              "      <td>0.306901</td>\n",
              "      <td>-0.008598</td>\n",
              "      <td>0.210741</td>\n",
              "      <td>-0.014197</td>\n",
              "      <td>0.102570</td>\n",
              "      <td>0.030423</td>\n",
              "      <td>0.249247</td>\n",
              "      <td>-0.002708</td>\n",
              "      <td>0.221406</td>\n",
              "      <td>-0.149129</td>\n",
              "      <td>0.109211</td>\n",
              "      <td>0.00153</td>\n",
              "      <td>-0.563941</td>\n",
              "      <td>-0.169730</td>\n",
              "      <td>0.042060</td>\n",
              "      <td>-0.065157</td>\n",
              "      <td>-0.052584</td>\n",
              "      <td>0.022522</td>\n",
              "      <td>-0.175324</td>\n",
              "      <td>-0.076534</td>\n",
              "      <td>-0.119258</td>\n",
              "      <td>-0.469808</td>\n",
              "      <td>-0.136817</td>\n",
              "      <td>-0.223019</td>\n",
              "      <td>-0.189165</td>\n",
              "      <td>-0.001855</td>\n",
              "      <td>-0.137473</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>0.502330</td>\n",
              "      <td>-0.103651</td>\n",
              "      <td>-0.057010</td>\n",
              "      <td>0.179594</td>\n",
              "      <td>0.254318</td>\n",
              "      <td>0.013924</td>\n",
              "      <td>0.035418</td>\n",
              "      <td>-0.090732</td>\n",
              "      <td>-0.016689</td>\n",
              "      <td>-0.439385</td>\n",
              "      <td>0.112429</td>\n",
              "      <td>-0.225728</td>\n",
              "      <td>0.032242</td>\n",
              "      <td>-0.150984</td>\n",
              "      <td>-0.028263</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>-0.061611</td>\n",
              "      <td>0.235810</td>\n",
              "      <td>-0.141131</td>\n",
              "      <td>0.309909</td>\n",
              "      <td>0.359485</td>\n",
              "      <td>-0.031121</td>\n",
              "      <td>0.386065</td>\n",
              "      <td>0.062337</td>\n",
              "      <td>0.221828</td>\n",
              "      <td>0.500230</td>\n",
              "      <td>0.386064</td>\n",
              "      <td>0.220311</td>\n",
              "      <td>0.410571</td>\n",
              "      <td>-0.147274</td>\n",
              "      <td>0.246684</td>\n",
              "      <td>-0.000364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60087</th>\n",
              "      <td>77674</td>\n",
              "      <td>0.074707</td>\n",
              "      <td>-0.088321</td>\n",
              "      <td>0.273382</td>\n",
              "      <td>0.332519</td>\n",
              "      <td>-0.001164</td>\n",
              "      <td>0.219474</td>\n",
              "      <td>-0.011959</td>\n",
              "      <td>0.116376</td>\n",
              "      <td>0.017980</td>\n",
              "      <td>0.293136</td>\n",
              "      <td>0.013737</td>\n",
              "      <td>0.264685</td>\n",
              "      <td>-0.108912</td>\n",
              "      <td>0.173457</td>\n",
              "      <td>0.00153</td>\n",
              "      <td>-0.546937</td>\n",
              "      <td>-0.174522</td>\n",
              "      <td>0.048707</td>\n",
              "      <td>-0.089601</td>\n",
              "      <td>-0.057659</td>\n",
              "      <td>0.001148</td>\n",
              "      <td>-0.193786</td>\n",
              "      <td>-0.114654</td>\n",
              "      <td>-0.144103</td>\n",
              "      <td>-0.580800</td>\n",
              "      <td>-0.154351</td>\n",
              "      <td>-0.247110</td>\n",
              "      <td>-0.205706</td>\n",
              "      <td>-0.024139</td>\n",
              "      <td>-0.177692</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>0.520364</td>\n",
              "      <td>-0.099815</td>\n",
              "      <td>-0.039613</td>\n",
              "      <td>0.183781</td>\n",
              "      <td>0.274860</td>\n",
              "      <td>-0.000015</td>\n",
              "      <td>0.025688</td>\n",
              "      <td>-0.126613</td>\n",
              "      <td>-0.027727</td>\n",
              "      <td>-0.562820</td>\n",
              "      <td>0.138785</td>\n",
              "      <td>-0.233372</td>\n",
              "      <td>0.058978</td>\n",
              "      <td>-0.133050</td>\n",
              "      <td>-0.004235</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>-0.026573</td>\n",
              "      <td>0.249229</td>\n",
              "      <td>-0.137028</td>\n",
              "      <td>0.362983</td>\n",
              "      <td>0.390177</td>\n",
              "      <td>-0.002312</td>\n",
              "      <td>0.413259</td>\n",
              "      <td>0.102695</td>\n",
              "      <td>0.260478</td>\n",
              "      <td>0.598779</td>\n",
              "      <td>0.447487</td>\n",
              "      <td>0.260847</td>\n",
              "      <td>0.470391</td>\n",
              "      <td>-0.084773</td>\n",
              "      <td>0.351150</td>\n",
              "      <td>-0.000364</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>60088 rows × 64 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       index   Hbo_Op1   Hbo_Op2  ...  Oxy_Op13  Oxy_Op14  Oxy_Op15\n",
              "0          0  0.024675 -0.015218  ... -0.030096 -0.049618 -0.000364\n",
              "1          1  0.030511 -0.012349  ... -0.030096 -0.078378 -0.000364\n",
              "2          2  0.039081 -0.004290  ... -0.030096 -0.094838 -0.000364\n",
              "3          3  0.050448  0.008479  ... -0.030096 -0.092341 -0.000364\n",
              "4          4  0.064245  0.024718  ... -0.030096 -0.067763 -0.000364\n",
              "...      ...       ...       ...  ...       ...       ...       ...\n",
              "60083  77670  0.034821 -0.115473  ... -0.158819  0.121507 -0.000364\n",
              "60084  77671  0.047532 -0.110554  ... -0.185784  0.128601 -0.000364\n",
              "60085  77672  0.057649 -0.105592  ... -0.181871  0.170950 -0.000364\n",
              "60086  77673  0.066079 -0.099071  ... -0.147274  0.246684 -0.000364\n",
              "60087  77674  0.074707 -0.088321  ... -0.084773  0.351150 -0.000364\n",
              "\n",
              "[60088 rows x 64 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 230
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yjc0BgRpnQxT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "1139b1b4-1fde-41f5-d964-03b45345824f"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.model_selection import cross_val_score, cross_validate\n",
        "from time import time\n",
        "\n",
        "s1 = time()\n",
        "selector = SelectFromModel(RandomForestClassifier()).fit(X_trainr, y_trainr)\n",
        "s2 = time()\n",
        "print(\"Passed time for Feature Selection:\",s2-s1)\n",
        "X_rnd_trainr = selector.transform(X_trainr)\n",
        "X_rnd_testr = selector.transform(X_testr)\n",
        "\n",
        "print(\"Selected Columns:\",X.loc[:,selector.get_support()].columns.tolist(),\"\\n\")\n",
        "s3 = time()\n",
        "models = RandomForestClassifier(random_state = 127)\n",
        "cv = cross_validate(models,X_rnd_trainr,y_trainr,cv = 3, n_jobs=-1, return_estimator=True)\n",
        "s4 = time()\n",
        "print(\"Mean training accuracy: {}\".format(np.mean(cv['test_score'])))\n",
        "print(\"Test accuracy: {}\".format(cv[\"estimator\"][0].score(X_rnd_testr,y_testr)))\n",
        "print(\"Passed time for Cross Validation:\",s4-s3)\n"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Passed time for Feature Selection: 57.61167597770691\n",
            "Selected Columns: ['Hbo_Op2', 'Hbo_Op4', 'Hbo_Op16', 'Hbr_Op1', 'Hbr_Op2', 'Hbr_Op3', 'Hbr_Op4', 'Hbr_Op5', 'Hbr_Op6', 'Hbr_Op7', 'Hbr_Op8', 'Hbr_Op9', 'Hbr_Op10', 'Hbr_Op12', 'Hbr_Op14', 'Hbr_Op16', 'Hbt_Op1', 'Hbt_Op2', 'Hbt_Op3', 'Hbt_Op4', 'Hbt_Op6', 'Hbt_Op7', 'Hbt_Op8', 'Hbt_Op9', 'Hbt_Op10', 'Hbt_Op11', 'Hbt_Op12', 'Hbt_Op13', 'Hbt_Op14', 'Hbt_Op16', 'Oxy_Op2', 'Oxy_Op3', 'Oxy_Op4'] \n",
            "\n",
            "Mean training accuracy: 0.7430872880950973\n",
            "Test accuracy: 0.7433294502690408\n",
            "Passed time for Cross Validation: 71.57149600982666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0IJE2QMqefG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
        "\n",
        "param_rnd = {\"min_samples_split\" : [2,5,10,20],\n",
        "             \"criterion\" : [\"gini\",\"entropy\"],\n",
        "             \"ccp_alpha\": [1e-5,1e-4,1e-3,0.01,0.1] }\n",
        "\n",
        "\n",
        "ran_forest = RandomForestClassifier(random_state = rnd)\n",
        "\n",
        "clf_rnd = GridSearchCV(ran_forest, param_rnd, cv=3, n_jobs = -1)\n",
        "clf_rnd.fit(X_rnd_train, y_train)\n",
        "\n",
        "print(\"Tuned Random Forest Classification Parameters: {}\".format(clf_rnd.best_params_)) \n",
        "print(\"Mean of the cv scores is {:.6f}\".format(clf_rnd.best_score_))\n",
        "print(\"Test Score {:.6f}\".format(clf_rnd.score(X_rnd_test,y_test)))\n",
        "print(\"Seconds used for refitting the best model on the train dataset: {:.6f}\".format(clf_rnd.refit_time_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2O_5bgwsCQq",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkrcdvCjLGEo",
        "colab_type": "text"
      },
      "source": [
        "## Neural Nets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OR-_OzXxLFfp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import tensorflow as tf\n",
        "\n",
        "#Plotting Function\n",
        "def plot_curve(epochs, hist, list_of_metrics):\n",
        "  plt.figure()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Value\")\n",
        "\n",
        "  for m in list_of_metrics:\n",
        "    x = hist[m]\n",
        "    plt.plot(epochs[1:], x[1:], label=m)\n",
        "\n",
        "  plt.legend()\n",
        "\n",
        "  #Model and Train Functions\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def create_model(my_learning_rate):\n",
        "  \"\"\"Create and compile a deep\n",
        "   neural net.\"\"\"\n",
        "  \n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(tf.keras.layers.Dense(units=256, input_dim=64))\n",
        "  model.add(tf.keras.layers.Dense(units=128, activation='linear',kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
        "  model.add(tf.keras.layers.Dense(units=3, activation='softmax'))  \n",
        "                           \n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(lr=my_learning_rate),\n",
        "                loss=\"categorical_crossentropy\",\n",
        "                metrics=['accuracy',\n",
        "                         tf.keras.metrics.AUC(name=\"auc\"),\n",
        "                         tf.keras.metrics.Precision(name=\"precision\")])\n",
        "  \n",
        "  return model  \n",
        "\n",
        "\n",
        "def train_model(model, train_features, train_label, epochs,\n",
        "                batch_size=None, validation_split=0.1):\n",
        "  \"\"\"Train the model by feeding it data.\"\"\"\n",
        "\n",
        "  history = model.fit(x=train_features, y=train_label, batch_size=batch_size,\n",
        "                      epochs=epochs, shuffle=True, \n",
        "                      validation_split=validation_split)\n",
        " \n",
        "  epochs = history.epoch\n",
        "  hist = pd.DataFrame(history.history)\n",
        "\n",
        "  return epochs, hist"
      ],
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLtUwALOLc24",
        "colab_type": "text"
      },
      "source": [
        "### ResponesCode Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y18TSWb-MBN9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4f013899-ef8a-4e90-8440-1741fd820d8f"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(48908, 63)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px2xMzzGLOJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_arr = np_utils.to_categorical(yr-1, num_classes=3)\n",
        "X_arr = MinMaxScaler().fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_arr, y_arr, test_size=0.3, random_state = rnd)"
      ],
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_4fR7p0LojC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1046f56c-89ba-49e6-d413-080d7b850302"
      },
      "source": [
        "# Hyperparameter tuning\n",
        "learning_rate = 0.02\n",
        "epochs = 100\n",
        "batch_size = 4000\n",
        "validation_split = 0.1\n",
        "\n",
        "\n",
        "# Establish the model's topography.\n",
        "my_model = create_model(learning_rate)\n",
        "\n",
        "# Train the model on the normalized training set.\n",
        "epochs, hist = train_model(my_model, X_train, y_train, \n",
        "                           epochs, batch_size, validation_split)\n",
        "\n",
        "# Plot a graph of the metric vs. epochs.\n",
        "list_of_metrics_to_plot = ['accuracy','auc','precision']\n",
        "plot_curve(epochs, hist, list_of_metrics_to_plot)\n",
        "\n",
        "# Evaluate against the test set.\n",
        "print(\"\\n Evaluate the new model against the test set:\")\n",
        "my_model.evaluate(x=X_test, y=y_test, batch_size=batch_size)"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "12/12 [==============================] - 1s 53ms/step - loss: 12.1540 - accuracy: 0.3567 - auc: 0.5390 - precision: 0.3493 - val_loss: 1.3600 - val_accuracy: 0.4657 - val_auc: 0.6700 - val_precision: 0.4657\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 3.6294 - accuracy: 0.3812 - auc: 0.5919 - precision: 0.3812 - val_loss: 3.4812 - val_accuracy: 0.4659 - val_auc: 0.6729 - val_precision: 0.4659\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.9360 - accuracy: 0.4208 - auc: 0.6324 - precision: 0.4157 - val_loss: 1.3185 - val_accuracy: 0.4649 - val_auc: 0.6700 - val_precision: 0.0000e+00\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.3185 - accuracy: 0.4369 - auc: 0.6475 - precision: 0.4409 - val_loss: 1.2839 - val_accuracy: 0.4437 - val_auc: 0.6606 - val_precision: 0.0000e+00\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.2611 - accuracy: 0.4457 - auc: 0.6621 - precision: 0.4501 - val_loss: 1.2375 - val_accuracy: 0.4181 - val_auc: 0.6492 - val_precision: 0.5833\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.2343 - accuracy: 0.4473 - auc: 0.6647 - precision: 0.4614 - val_loss: 1.2215 - val_accuracy: 0.4649 - val_auc: 0.6740 - val_precision: 0.5200\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.2203 - accuracy: 0.4460 - auc: 0.6645 - precision: 0.4450 - val_loss: 1.2129 - val_accuracy: 0.4538 - val_auc: 0.6628 - val_precision: 1.0000\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.2059 - accuracy: 0.4474 - auc: 0.6656 - precision: 0.5032 - val_loss: 1.2064 - val_accuracy: 0.4659 - val_auc: 0.6748 - val_precision: 0.4691\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.1984 - accuracy: 0.4424 - auc: 0.6632 - precision: 0.4721 - val_loss: 1.1868 - val_accuracy: 0.4659 - val_auc: 0.6739 - val_precision: 0.5000\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.1820 - accuracy: 0.4547 - auc: 0.6673 - precision: 0.5029 - val_loss: 1.1753 - val_accuracy: 0.4647 - val_auc: 0.6747 - val_precision: 0.0000e+00\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 1.1699 - accuracy: 0.4559 - auc: 0.6690 - precision: 0.4769 - val_loss: 1.1637 - val_accuracy: 0.4659 - val_auc: 0.6739 - val_precision: 0.6000\n",
            "Epoch 12/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 1.1584 - accuracy: 0.4588 - auc: 0.6713 - precision: 0.5185 - val_loss: 1.1531 - val_accuracy: 0.4689 - val_auc: 0.6755 - val_precision: 0.0000e+00\n",
            "Epoch 13/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.1484 - accuracy: 0.4578 - auc: 0.6711 - precision: 0.3333 - val_loss: 1.1423 - val_accuracy: 0.4665 - val_auc: 0.6753 - val_precision: 0.4267\n",
            "Epoch 14/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 1.1398 - accuracy: 0.4519 - auc: 0.6671 - precision: 0.4973 - val_loss: 1.1328 - val_accuracy: 0.4659 - val_auc: 0.6740 - val_precision: 0.4083\n",
            "Epoch 15/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 1.1284 - accuracy: 0.4575 - auc: 0.6702 - precision: 0.5302 - val_loss: 1.1228 - val_accuracy: 0.4685 - val_auc: 0.6741 - val_precision: 0.3333\n",
            "Epoch 16/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.1190 - accuracy: 0.4572 - auc: 0.6711 - precision: 0.3519 - val_loss: 1.1154 - val_accuracy: 0.4621 - val_auc: 0.6752 - val_precision: 0.0000e+00\n",
            "Epoch 17/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.1105 - accuracy: 0.4587 - auc: 0.6704 - precision: 0.4737 - val_loss: 1.1073 - val_accuracy: 0.4593 - val_auc: 0.6723 - val_precision: 0.0000e+00\n",
            "Epoch 18/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 1.1023 - accuracy: 0.4577 - auc: 0.6699 - precision: 0.4667 - val_loss: 1.0994 - val_accuracy: 0.4417 - val_auc: 0.6640 - val_precision: 0.0000e+00\n",
            "Epoch 19/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.0946 - accuracy: 0.4528 - auc: 0.6695 - precision: 0.5000 - val_loss: 1.0900 - val_accuracy: 0.4671 - val_auc: 0.6751 - val_precision: 0.0000e+00\n",
            "Epoch 20/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.0876 - accuracy: 0.4519 - auc: 0.6686 - precision: 0.5135 - val_loss: 1.0831 - val_accuracy: 0.4695 - val_auc: 0.6756 - val_precision: 0.0000e+00\n",
            "Epoch 21/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.0794 - accuracy: 0.4605 - auc: 0.6722 - precision: 0.5078 - val_loss: 1.0776 - val_accuracy: 0.4689 - val_auc: 0.6745 - val_precision: 0.4000\n",
            "Epoch 22/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.0729 - accuracy: 0.4563 - auc: 0.6716 - precision: 0.5278 - val_loss: 1.0701 - val_accuracy: 0.4585 - val_auc: 0.6735 - val_precision: 0.0000e+00\n",
            "Epoch 23/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 1.0667 - accuracy: 0.4574 - auc: 0.6704 - precision: 0.4706 - val_loss: 1.0636 - val_accuracy: 0.4627 - val_auc: 0.6752 - val_precision: 0.0000e+00\n",
            "Epoch 24/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.0611 - accuracy: 0.4587 - auc: 0.6707 - precision: 0.4141 - val_loss: 1.0617 - val_accuracy: 0.4659 - val_auc: 0.6752 - val_precision: 0.0000e+00\n",
            "Epoch 25/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.0564 - accuracy: 0.4513 - auc: 0.6679 - precision: 0.4706 - val_loss: 1.0518 - val_accuracy: 0.4679 - val_auc: 0.6751 - val_precision: 0.0000e+00\n",
            "Epoch 26/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 1.0499 - accuracy: 0.4579 - auc: 0.6714 - precision: 0.5484 - val_loss: 1.0501 - val_accuracy: 0.4659 - val_auc: 0.6750 - val_precision: 0.4439\n",
            "Epoch 27/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.0469 - accuracy: 0.4496 - auc: 0.6670 - precision: 0.4760 - val_loss: 1.0448 - val_accuracy: 0.4673 - val_auc: 0.6756 - val_precision: 0.0000e+00\n",
            "Epoch 28/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.0416 - accuracy: 0.4517 - auc: 0.6688 - precision: 0.4495 - val_loss: 1.0379 - val_accuracy: 0.4681 - val_auc: 0.6752 - val_precision: 0.0000e+00\n",
            "Epoch 29/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.0383 - accuracy: 0.4547 - auc: 0.6699 - precision: 0.4000 - val_loss: 1.0360 - val_accuracy: 0.4635 - val_auc: 0.6739 - val_precision: 0.0000e+00\n",
            "Epoch 30/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.0336 - accuracy: 0.4517 - auc: 0.6676 - precision: 0.6250 - val_loss: 1.0341 - val_accuracy: 0.4375 - val_auc: 0.6589 - val_precision: 0.0000e+00\n",
            "Epoch 31/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.0286 - accuracy: 0.4572 - auc: 0.6719 - precision: 0.0000e+00 - val_loss: 1.0277 - val_accuracy: 0.4659 - val_auc: 0.6752 - val_precision: 0.0000e+00\n",
            "Epoch 32/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.0252 - accuracy: 0.4585 - auc: 0.6714 - precision: 0.0000e+00 - val_loss: 1.0230 - val_accuracy: 0.4663 - val_auc: 0.6747 - val_precision: 0.0000e+00\n",
            "Epoch 33/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 1.0219 - accuracy: 0.4570 - auc: 0.6713 - precision: 0.0000e+00 - val_loss: 1.0203 - val_accuracy: 0.4669 - val_auc: 0.6758 - val_precision: 0.0000e+00\n",
            "Epoch 34/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.0185 - accuracy: 0.4582 - auc: 0.6718 - precision: 0.0000e+00 - val_loss: 1.0181 - val_accuracy: 0.4671 - val_auc: 0.6753 - val_precision: 0.0000e+00\n",
            "Epoch 35/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 1.0162 - accuracy: 0.4574 - auc: 0.6715 - precision: 1.0000 - val_loss: 1.0144 - val_accuracy: 0.4665 - val_auc: 0.6752 - val_precision: 0.0000e+00\n",
            "Epoch 36/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.0142 - accuracy: 0.4567 - auc: 0.6712 - precision: 0.0000e+00 - val_loss: 1.0122 - val_accuracy: 0.4673 - val_auc: 0.6755 - val_precision: 0.0000e+00\n",
            "Epoch 37/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.0128 - accuracy: 0.4544 - auc: 0.6690 - precision: 0.7273 - val_loss: 1.0110 - val_accuracy: 0.4550 - val_auc: 0.6705 - val_precision: 0.0000e+00\n",
            "Epoch 38/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.0088 - accuracy: 0.4585 - auc: 0.6706 - precision: 0.4000 - val_loss: 1.0092 - val_accuracy: 0.4677 - val_auc: 0.6756 - val_precision: 0.0000e+00\n",
            "Epoch 39/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.0064 - accuracy: 0.4594 - auc: 0.6712 - precision: 0.7500 - val_loss: 1.0057 - val_accuracy: 0.4679 - val_auc: 0.6752 - val_precision: 0.0000e+00\n",
            "Epoch 40/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 1.0077 - accuracy: 0.4540 - auc: 0.6681 - precision: 0.4333 - val_loss: 1.0056 - val_accuracy: 0.4659 - val_auc: 0.6750 - val_precision: 0.3939\n",
            "Epoch 41/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 1.0030 - accuracy: 0.4562 - auc: 0.6707 - precision: 0.4776 - val_loss: 1.0067 - val_accuracy: 0.4683 - val_auc: 0.6753 - val_precision: 0.0000e+00\n",
            "Epoch 42/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.0022 - accuracy: 0.4576 - auc: 0.6707 - precision: 0.0000e+00 - val_loss: 1.0010 - val_accuracy: 0.4659 - val_auc: 0.6756 - val_precision: 0.0000e+00\n",
            "Epoch 43/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.0000 - accuracy: 0.4583 - auc: 0.6698 - precision: 0.0000e+00 - val_loss: 0.9983 - val_accuracy: 0.4671 - val_auc: 0.6756 - val_precision: 0.0000e+00\n",
            "Epoch 44/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.9973 - accuracy: 0.4582 - auc: 0.6720 - precision: 1.0000 - val_loss: 0.9968 - val_accuracy: 0.4659 - val_auc: 0.6755 - val_precision: 0.0000e+00\n",
            "Epoch 45/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.9960 - accuracy: 0.4590 - auc: 0.6721 - precision: 1.0000 - val_loss: 0.9993 - val_accuracy: 0.4673 - val_auc: 0.6744 - val_precision: 0.0000e+00\n",
            "Epoch 46/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9960 - accuracy: 0.4531 - auc: 0.6690 - precision: 1.0000 - val_loss: 0.9944 - val_accuracy: 0.4663 - val_auc: 0.6754 - val_precision: 0.0000e+00\n",
            "Epoch 47/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9939 - accuracy: 0.4601 - auc: 0.6712 - precision: 0.0000e+00 - val_loss: 0.9937 - val_accuracy: 0.4679 - val_auc: 0.6755 - val_precision: 0.0000e+00\n",
            "Epoch 48/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.9921 - accuracy: 0.4585 - auc: 0.6737 - precision: 0.0000e+00 - val_loss: 0.9926 - val_accuracy: 0.4673 - val_auc: 0.6759 - val_precision: 0.0000e+00\n",
            "Epoch 49/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9912 - accuracy: 0.4590 - auc: 0.6732 - precision: 1.0000 - val_loss: 0.9910 - val_accuracy: 0.4677 - val_auc: 0.6757 - val_precision: 0.0000e+00\n",
            "Epoch 50/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.9905 - accuracy: 0.4594 - auc: 0.6719 - precision: 0.5333 - val_loss: 0.9909 - val_accuracy: 0.4659 - val_auc: 0.6763 - val_precision: 0.0000e+00\n",
            "Epoch 51/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9904 - accuracy: 0.4506 - auc: 0.6691 - precision: 1.0000 - val_loss: 0.9902 - val_accuracy: 0.4681 - val_auc: 0.6747 - val_precision: 0.0000e+00\n",
            "Epoch 52/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.9888 - accuracy: 0.4581 - auc: 0.6725 - precision: 0.0000e+00 - val_loss: 0.9885 - val_accuracy: 0.4675 - val_auc: 0.6752 - val_precision: 0.0000e+00\n",
            "Epoch 53/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.9879 - accuracy: 0.4579 - auc: 0.6721 - precision: 0.0000e+00 - val_loss: 0.9877 - val_accuracy: 0.4675 - val_auc: 0.6750 - val_precision: 0.0000e+00\n",
            "Epoch 54/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.9875 - accuracy: 0.4589 - auc: 0.6723 - precision: 0.0000e+00 - val_loss: 0.9900 - val_accuracy: 0.4675 - val_auc: 0.6758 - val_precision: 0.0000e+00\n",
            "Epoch 55/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9872 - accuracy: 0.4585 - auc: 0.6725 - precision: 1.0000 - val_loss: 0.9873 - val_accuracy: 0.4665 - val_auc: 0.6756 - val_precision: 0.0000e+00\n",
            "Epoch 56/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9869 - accuracy: 0.4531 - auc: 0.6704 - precision: 0.6154 - val_loss: 0.9927 - val_accuracy: 0.4659 - val_auc: 0.6759 - val_precision: 0.0000e+00\n",
            "Epoch 57/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.9899 - accuracy: 0.4472 - auc: 0.6667 - precision: 0.3333 - val_loss: 0.9928 - val_accuracy: 0.4564 - val_auc: 0.6698 - val_precision: 0.0000e+00\n",
            "Epoch 58/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9873 - accuracy: 0.4576 - auc: 0.6695 - precision: 0.0000e+00 - val_loss: 0.9860 - val_accuracy: 0.4659 - val_auc: 0.6757 - val_precision: 0.0000e+00\n",
            "Epoch 59/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.9867 - accuracy: 0.4585 - auc: 0.6690 - precision: 0.4967 - val_loss: 0.9876 - val_accuracy: 0.4675 - val_auc: 0.6752 - val_precision: 0.0000e+00\n",
            "Epoch 60/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.9854 - accuracy: 0.4529 - auc: 0.6686 - precision: 0.5789 - val_loss: 0.9848 - val_accuracy: 0.4669 - val_auc: 0.6749 - val_precision: 0.0000e+00\n",
            "Epoch 61/100\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.9841 - accuracy: 0.4579 - auc: 0.6718 - precision: 0.0000e+00 - val_loss: 0.9853 - val_accuracy: 0.4459 - val_auc: 0.6652 - val_precision: 0.0000e+00\n",
            "Epoch 62/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.9834 - accuracy: 0.4569 - auc: 0.6720 - precision: 0.0000e+00 - val_loss: 0.9830 - val_accuracy: 0.4659 - val_auc: 0.6755 - val_precision: 0.0000e+00\n",
            "Epoch 63/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.9826 - accuracy: 0.4584 - auc: 0.6729 - precision: 0.0000e+00 - val_loss: 0.9835 - val_accuracy: 0.4673 - val_auc: 0.6756 - val_precision: 0.0000e+00\n",
            "Epoch 64/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.9824 - accuracy: 0.4584 - auc: 0.6728 - precision: 0.0000e+00 - val_loss: 0.9830 - val_accuracy: 0.4663 - val_auc: 0.6755 - val_precision: 0.0000e+00\n",
            "Epoch 65/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.9823 - accuracy: 0.4579 - auc: 0.6720 - precision: 0.0000e+00 - val_loss: 0.9824 - val_accuracy: 0.4659 - val_auc: 0.6753 - val_precision: 0.0000e+00\n",
            "Epoch 66/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9821 - accuracy: 0.4591 - auc: 0.6718 - precision: 0.0000e+00 - val_loss: 0.9821 - val_accuracy: 0.4675 - val_auc: 0.6756 - val_precision: 0.0000e+00\n",
            "Epoch 67/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9819 - accuracy: 0.4587 - auc: 0.6717 - precision: 0.5000 - val_loss: 0.9835 - val_accuracy: 0.4381 - val_auc: 0.6603 - val_precision: 0.0000e+00\n",
            "Epoch 68/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.9815 - accuracy: 0.4562 - auc: 0.6718 - precision: 0.0000e+00 - val_loss: 0.9822 - val_accuracy: 0.4671 - val_auc: 0.6761 - val_precision: 0.0000e+00\n",
            "Epoch 69/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.9812 - accuracy: 0.4582 - auc: 0.6719 - precision: 0.0000e+00 - val_loss: 0.9826 - val_accuracy: 0.4669 - val_auc: 0.6755 - val_precision: 0.0000e+00\n",
            "Epoch 70/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.9810 - accuracy: 0.4584 - auc: 0.6720 - precision: 0.0000e+00 - val_loss: 0.9810 - val_accuracy: 0.4677 - val_auc: 0.6762 - val_precision: 0.0000e+00\n",
            "Epoch 71/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.9803 - accuracy: 0.4589 - auc: 0.6740 - precision: 0.0000e+00 - val_loss: 0.9810 - val_accuracy: 0.4673 - val_auc: 0.6753 - val_precision: 0.0000e+00\n",
            "Epoch 72/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9803 - accuracy: 0.4585 - auc: 0.6735 - precision: 0.0000e+00 - val_loss: 0.9808 - val_accuracy: 0.4675 - val_auc: 0.6759 - val_precision: 0.0000e+00\n",
            "Epoch 73/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.9804 - accuracy: 0.4579 - auc: 0.6721 - precision: 0.0000e+00 - val_loss: 0.9817 - val_accuracy: 0.4673 - val_auc: 0.6759 - val_precision: 0.0000e+00\n",
            "Epoch 74/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9812 - accuracy: 0.4566 - auc: 0.6700 - precision: 0.0000e+00 - val_loss: 0.9802 - val_accuracy: 0.4673 - val_auc: 0.6756 - val_precision: 0.0000e+00\n",
            "Epoch 75/100\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.9805 - accuracy: 0.4587 - auc: 0.6720 - precision: 0.0000e+00 - val_loss: 0.9808 - val_accuracy: 0.4673 - val_auc: 0.6754 - val_precision: 0.0000e+00\n",
            "Epoch 76/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.9817 - accuracy: 0.4570 - auc: 0.6709 - precision: 0.0000e+00 - val_loss: 0.9802 - val_accuracy: 0.4673 - val_auc: 0.6759 - val_precision: 0.0000e+00\n",
            "Epoch 77/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.9796 - accuracy: 0.4591 - auc: 0.6731 - precision: 0.0000e+00 - val_loss: 0.9811 - val_accuracy: 0.4665 - val_auc: 0.6759 - val_precision: 0.0000e+00\n",
            "Epoch 78/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.9802 - accuracy: 0.4582 - auc: 0.6717 - precision: 0.0000e+00 - val_loss: 0.9813 - val_accuracy: 0.4661 - val_auc: 0.6754 - val_precision: 0.0000e+00\n",
            "Epoch 79/100\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.9797 - accuracy: 0.4582 - auc: 0.6722 - precision: 0.0000e+00 - val_loss: 0.9813 - val_accuracy: 0.4669 - val_auc: 0.6753 - val_precision: 0.0000e+00\n",
            "Epoch 80/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.9796 - accuracy: 0.4604 - auc: 0.6728 - precision: 0.0000e+00 - val_loss: 0.9804 - val_accuracy: 0.4673 - val_auc: 0.6755 - val_precision: 0.0000e+00\n",
            "Epoch 81/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9793 - accuracy: 0.4592 - auc: 0.6725 - precision: 0.0000e+00 - val_loss: 0.9795 - val_accuracy: 0.4665 - val_auc: 0.6753 - val_precision: 0.0000e+00\n",
            "Epoch 82/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.9790 - accuracy: 0.4589 - auc: 0.6735 - precision: 0.0000e+00 - val_loss: 0.9799 - val_accuracy: 0.4669 - val_auc: 0.6756 - val_precision: 0.0000e+00\n",
            "Epoch 83/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.9791 - accuracy: 0.4579 - auc: 0.6733 - precision: 0.0000e+00 - val_loss: 0.9793 - val_accuracy: 0.4679 - val_auc: 0.6751 - val_precision: 0.0000e+00\n",
            "Epoch 84/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9789 - accuracy: 0.4588 - auc: 0.6731 - precision: 0.0000e+00 - val_loss: 0.9791 - val_accuracy: 0.4669 - val_auc: 0.6754 - val_precision: 0.0000e+00\n",
            "Epoch 85/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9790 - accuracy: 0.4587 - auc: 0.6726 - precision: 0.0000e+00 - val_loss: 0.9794 - val_accuracy: 0.4677 - val_auc: 0.6762 - val_precision: 0.0000e+00\n",
            "Epoch 86/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.9791 - accuracy: 0.4576 - auc: 0.6729 - precision: 0.0000e+00 - val_loss: 0.9793 - val_accuracy: 0.4671 - val_auc: 0.6764 - val_precision: 0.0000e+00\n",
            "Epoch 87/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.9788 - accuracy: 0.4590 - auc: 0.6728 - precision: 0.0000e+00 - val_loss: 0.9802 - val_accuracy: 0.4558 - val_auc: 0.6719 - val_precision: 0.0000e+00\n",
            "Epoch 88/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.9786 - accuracy: 0.4575 - auc: 0.6728 - precision: 0.0000e+00 - val_loss: 0.9789 - val_accuracy: 0.4677 - val_auc: 0.6762 - val_precision: 0.0000e+00\n",
            "Epoch 89/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.9785 - accuracy: 0.4581 - auc: 0.6739 - precision: 0.0000e+00 - val_loss: 0.9793 - val_accuracy: 0.4667 - val_auc: 0.6760 - val_precision: 0.0000e+00\n",
            "Epoch 90/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9788 - accuracy: 0.4574 - auc: 0.6727 - precision: 0.0000e+00 - val_loss: 0.9804 - val_accuracy: 0.4677 - val_auc: 0.6762 - val_precision: 0.0000e+00\n",
            "Epoch 91/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9790 - accuracy: 0.4579 - auc: 0.6718 - precision: 0.0000e+00 - val_loss: 0.9798 - val_accuracy: 0.4667 - val_auc: 0.6755 - val_precision: 0.0000e+00\n",
            "Epoch 92/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.9792 - accuracy: 0.4586 - auc: 0.6722 - precision: 0.0000e+00 - val_loss: 0.9791 - val_accuracy: 0.4659 - val_auc: 0.6757 - val_precision: 0.0000e+00\n",
            "Epoch 93/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.9781 - accuracy: 0.4593 - auc: 0.6748 - precision: 0.0000e+00 - val_loss: 0.9789 - val_accuracy: 0.4663 - val_auc: 0.6763 - val_precision: 0.0000e+00\n",
            "Epoch 94/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9787 - accuracy: 0.4586 - auc: 0.6733 - precision: 0.0000e+00 - val_loss: 0.9790 - val_accuracy: 0.4669 - val_auc: 0.6755 - val_precision: 0.0000e+00\n",
            "Epoch 95/100\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.9782 - accuracy: 0.4587 - auc: 0.6738 - precision: 0.0000e+00 - val_loss: 0.9788 - val_accuracy: 0.4671 - val_auc: 0.6756 - val_precision: 0.0000e+00\n",
            "Epoch 96/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9784 - accuracy: 0.4582 - auc: 0.6730 - precision: 0.0000e+00 - val_loss: 0.9792 - val_accuracy: 0.4673 - val_auc: 0.6754 - val_precision: 0.0000e+00\n",
            "Epoch 97/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.9783 - accuracy: 0.4584 - auc: 0.6728 - precision: 0.0000e+00 - val_loss: 0.9794 - val_accuracy: 0.4671 - val_auc: 0.6752 - val_precision: 0.0000e+00\n",
            "Epoch 98/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.9792 - accuracy: 0.4577 - auc: 0.6729 - precision: 0.0000e+00 - val_loss: 0.9793 - val_accuracy: 0.4659 - val_auc: 0.6758 - val_precision: 0.0000e+00\n",
            "Epoch 99/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.9785 - accuracy: 0.4584 - auc: 0.6726 - precision: 0.0000e+00 - val_loss: 0.9789 - val_accuracy: 0.4679 - val_auc: 0.6760 - val_precision: 0.0000e+00\n",
            "Epoch 100/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9783 - accuracy: 0.4582 - auc: 0.6737 - precision: 0.0000e+00 - val_loss: 0.9790 - val_accuracy: 0.4659 - val_auc: 0.6757 - val_precision: 0.0000e+00\n",
            "\n",
            " Evaluate the new model against the test set:\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.9782 - accuracy: 0.4609 - auc: 0.6760 - precision: 0.0000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9781965613365173, 0.4609375, 0.6760416030883789, 0.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 243
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MssBirnsLjow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}